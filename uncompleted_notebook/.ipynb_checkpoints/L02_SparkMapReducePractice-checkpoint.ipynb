{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark RDD MapReduce分布式算法实践\n",
    "本实践模块包含两个算法，分别是\n",
    "1. WordCount词频统计算法\n",
    "2. 倒排索引算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('init complete\\xef\\xbc\\x9asc = ', <SparkContext master=local appName=wordCount>)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time         : 2017/12/26 13:32\n",
    "# @Author       : zenghao\n",
    "\n",
    "from pyspark import SparkContext, SparkConf # 导入相关工具包\n",
    "\n",
    "# 初始化Spark上下文\n",
    "# local为本地调试模式，具体集群方式参照http://spark.apache.org/docs/latest/cluster-overview.html\n",
    "conf = SparkConf().setAppName(\"wordCount\").setMaster(\"local\") \n",
    "sc = SparkContext(conf=conf)\n",
    "print (\"init complete：sc = \", sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).\n",
      "All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n",
      "By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.\n",
      "Local vs. cluster modes\n",
      "The behavior of the above code is undefined, and may not work as intended. To execute jobs, Spark breaks up the processing of RDD operations into tasks, each of which is executed by an executor. Prior to execution, Spark computes the task’s closure. The closure is those variables and methods which must be visible for the executor to perform its computations on the RDD (in this case foreach()). This closure is serialized and sent to each executor.\n",
      "The variables within the closure sent to each executor are now copies and thus, when counter is referenced within the foreach function, it’s no longer the counter on the driver node. There is still a counter in the memory of the driver node but this is no longer visible to the executors! The executors only see the copy from the serialized closure. Thus, the final value of counter will still be zero since all operations on counter were referencing the value within the serialized closure.\n",
      "In local mode, in some circumstances the foreach function will actually execute within the same JVM as the driver and will reference the same original counter, and may actually update it.\n",
      "To ensure well-defined behavior in these sorts of scenarios one should use an Accumulator. Accumulators in Spark are used specifically to provide a mechanism for safely updating a variable when execution is split up across worker nodes in a cluster. The Accumulators section of this guide discusses these in more detail.\n",
      "In general, closures - constructs like loops or locally defined methods, should not be used to mutate some global state. Spark does not define or guarantee the behavior of mutations to objects referenced from outside of closures. Some code that does this may work in local mode, but that’s just by accident and such code will not behave as expected in distributed mode. Use an Accumulator instead if some global aggregation is needed.\n",
      "Printing elements of an RDD\n",
      "Another common idiom is attempting to print out the elements of an RDD using rdd.foreach(println) or rdd.map(println). On a single machine, this will generate the expected output and print all the RDD’s elements. However, in cluster mode, the output to stdout being called by the executors is now writing to the executor’s stdout instead, not the one on the driver, so stdout on the driver won’t show these! To print all elements on the driver, one can use the collect() method to first bring the RDD to the driver node thus: rdd.collect().foreach(println). This can cause the driver to run out of memory, though, because collect() fetches the entire RDD to a single machine; if you only need to print a few elements of the RDD, a safer approach is to use the take(): rdd.take(100).foreach(println).\n"
     ]
    }
   ],
   "source": [
    "# fileData = sc.textFile(\"data/wordcount.txt\")  # 不设置前缀默认从hdfs上加载\n",
    "fileData = sc.textFile(\"file:///root/notebook/data/wordcount.txt\")  # 从本地文件系统读取\n",
    "for line in fileData.collect():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 实现mapReduce操作，统计文本词频，每行的格式为单词\\t词频\n",
    "\n",
    "count = None\n",
    "# Add your code here\n",
    "\"\"\"\n",
    "输出数据形如：\n",
    "operations: 2\n",
    "RDDs: 1\n",
    "all: 4\n",
    "code: 3\n",
    "JVM: 1\n",
    "\"\"\"\n",
    "\n",
    "# driver 获取数据， 如果数据量太大， 不建议用collect，可以用first 等等别的操作\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 倒排索引的一些基本名词\n",
    "\n",
    "1. 文档(Document)：一般搜索引擎的处理对象是互联网网页，而文档这个概念要更宽泛些，代表以文本形式存在的存储对象，相比网页来说，涵盖更多种形式，比如Word，PDF，html，XML等不同格式的文件都可以称之为文档。再比如一封邮件，一条短信，一条微博也可以称之为文档。在本书后续内容，很多情况下会使用文档来表征文本信息。\n",
    "2. 文档集合(Document Collection)：由若干文档构成的集合称之为文档集合。比如海量的互联网网页或者说大量的电子邮件都是文档集合的具体例子。\n",
    "3. 文档编号(Document ID)：在搜索引擎内部，会将文档集合内每个文档赋予一个唯一的内部编号，以此编号来作为这个文档的唯一标识，这样方便内部处理，每个文档的内部编号即称之为“文档编号”，后文有时会用DocID来便捷地代表文档编号。\n",
    "4. 单词编号(Word ID)：与文档编号类似，搜索引擎内部以唯一的编号来表征某个单词，单词编号可以作为某个单词的唯一表征。\n",
    "5. 倒排索引(Inverted Index)：倒排索引是实现“单词-文档矩阵”的一种具体存储形式，通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。倒排索引主要由两个部分组成：“单词词典”和“倒排文件”。\n",
    "6. 单词词典(Lexicon)：搜索引擎的通常索引单位是单词，单词词典是由文档集合中出现过的所有单词构成的字符串集合，单词词典内每条索引项记载单词本身的一些信息以及指向“倒排列表”的指针。\n",
    "7. 倒排列表(PostingList)：倒排列表记载了出现过某个单词的所有文档的文档列表及单词在该文档中出现的位置信息，每条记录称为一个倒排项(Posting)。根据倒排列表，即可获知哪些文档包含某个单词。\n",
    "8. 倒排文件(Inverted File)：所有单词的倒排列表往往顺序地存储在磁盘的某个文件里，这个文件即被称之为倒排文件，倒排文件是存储倒排索引的物理文件。\n",
    "\n",
    "> 关于此算法的具体介绍可参考[搜索引擎-倒排索引基础知识 ](http://blog.csdn.net/hguisu/article/details/7962350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'file:/root/notebook/data/invertedIndex/id1', u'hello world\\nhello hadoop\\nhadoop love\\nlove cat\\ncat love rabbit\\n')\n",
      "(u'file:/root/notebook/data/invertedIndex/id4', u'spark is better than hadoop\\nI love hadoop\\nyou love spark\\ncat and rabbit love app\\n')\n",
      "(u'file:/root/notebook/data/invertedIndex/id3', u'hello cat\\nhello rabbit\\ncat is doing spark\\ncat is the world\\nspark is good\\n')\n",
      "(u'file:/root/notebook/data/invertedIndex/id2', u'hello spark\\nthe spark app for you\\nI love you\\nyou are a cat\\n')\n",
      "(u'file:/root/notebook/data/invertedIndex/id5', u'hadoop is good in world\\nspark is very famous in world\\nI am rabbit\\nyou are cat\\n')\n"
     ]
    }
   ],
   "source": [
    "# 读取该目录下所有文件\n",
    "fileData = sc.wholeTextFiles(\"file:///root/notebook/data/invertedIndex\")  # 从本地文件系统读取\n",
    "for line in fileData.collect():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'id1', u'hello world\\nhello hadoop\\nhadoop love\\nlove cat\\ncat love rabbit\\n'), (u'id4', u'spark is better than hadoop\\nI love hadoop\\nyou love spark\\ncat and rabbit love app\\n'), (u'id3', u'hello cat\\nhello rabbit\\ncat is doing spark\\ncat is the world\\nspark is good\\n'), (u'id2', u'hello spark\\nthe spark app for you\\nI love you\\nyou are a cat\\n'), (u'id5', u'hadoop is good in world\\nspark is very famous in world\\nI am rabbit\\nyou are cat\\n')]\n"
     ]
    }
   ],
   "source": [
    "# 过滤文件路径，保留文档名\n",
    "data = fileData.map(lambda x: ((x[0].split(\"/\")[-1]), x[1])).collect()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现倒排索引算法，输出每行满足格式：\n",
    "# > 单词\\t所在文档1:词频\\t所在文档2:词频\\t所在文档3:词频\n",
    "# 其中每行按单词按字典自然顺序排列，所在文档按词频大小从左到右排列\n",
    "# 具体标准格式参照distData\n",
    "\n",
    "words = None\n",
    "# Add your code here\n",
    "\n",
    "distData = [(u'I', [u'id4:1', u'id2:1', u'id5:1']), (u'a', [u'id2:1']), (u'am', [u'id5:1']), (u'and', [u'id4:1']),\n",
    "            (u'app', [u'id4:1', u'id2:1']), (u'are', [u'id2:1', u'id5:1']), (u'better', [u'id4:1']),\n",
    "            (u'cat', [u'id4:1', u'id2:1', u'id5:1', u'id1:2', u'id3:3']), (u'doing', [u'id3:1']),\n",
    "            (u'famous', [u'id5:1']), (u'for', [u'id2:1']), (u'good', [u'id3:1', u'id5:1']),\n",
    "            (u'hadoop', [u'id5:1', u'id1:2', u'id4:2']), (u'hello', [u'id2:1', u'id1:2', u'id3:2']),\n",
    "            (u'in', [u'id5:2']), (u'is', [u'id4:1', u'id5:2', u'id3:3']), (u'love', [u'id2:1', u'id1:3', u'id4:3']),\n",
    "            (u'rabbit', [u'id1:1', u'id4:1', u'id3:1', u'id5:1']), (u'spark', [u'id5:1', u'id4:2', u'id3:2', u'id2:2']),\n",
    "            (u'than', [u'id4:1']), (u'the', [u'id3:1', u'id2:1']), (u'very', [u'id5:1']),\n",
    "            (u'world', [u'id1:1', u'id3:1', u'id5:2']), (u'you', [u'id4:1', u'id5:1', u'id2:3'])]\n",
    "print(\"words == distData ? \", words == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
