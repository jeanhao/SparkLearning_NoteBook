{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于Spark内置的ALS算法构建推荐模型，进行实时推荐demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf # 导入相关工具包\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "from math import sqrt\n",
    "from operator import add\n",
    "import itertools\n",
    "\n",
    "# 下面我们先来看系统整体的构建运行脉络\n",
    "\n",
    "# 方法模板\n",
    "def initSpark(): pass  # 初试化Spark上下文\n",
    "def parseData(sparkContext, ratingsFile, moviesFile): return (None, None, None, None)  # 解析预处理数据\n",
    "def trainModel(trainingData, validationData, testData, iterations=5, lambda_=0.01, blocks=-1): pass  # 训练得到最佳模型\n",
    "def predict(model, moviesData, rating, user_id): pass  # 根据模型和测试数据给用户进行推荐\n",
    "\n",
    "\n",
    "# 模型构建运行脉络\n",
    "sc = initSpark()  # 初试化Spark上下文\n",
    "trainingData, validationData, testData, moviesData = parseData(sc, \"file:///root/notebook/data/ratings.dat\",\n",
    "                                                               \"file:///root/notebook/data/movies.dat\")  # 解析预处理数据\n",
    "model = trainModel(trainingData, validationData, testData)  # 得到最佳模型\n",
    "predict(model, moviesData, testData, 1)  # 根据模型和测试数据给用户进行推荐\n",
    "# sc.stop()  # 终止回收Spark上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化Spark上下文\n",
    "# local为本地调试模式，具体集群方式参照http://spark.apache.org/docs/latest/cluster-overview.html\n",
    "def initSpark():\n",
    "    conf = SparkConf().setAppName(\"CF\").setMaster(\"local\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "    print (\"init complete : sc = \", sc)\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(sparkContext, ratingsFile, moviesFile):\n",
    "    # 导入数据，数据格式为：user_id::movies_id::rating::time\n",
    "    ratings = sparkContext.textFile(ratingsFile).map(lambda line: line.strip().split(\"::\"))\n",
    "    print(\"data.count() = %d\" % ratings.count())\n",
    "    # 对应的电影文件的格式为movieId::movieTitle\n",
    "    movies = sparkContext.textFile(moviesFile).map(lambda line: line.strip().split(\"::\"))\n",
    "\n",
    "    # 数据预处理，根据评论时间戳最后一位把整个数据集分成训练集(60%), 交叉验证集(20%), 和评估集(20%)\n",
    "    ratingsData = ratings.map(lambda fields: (long(fields[3]) % 10, (int(fields[0]), int(fields[1]), float(fields[2]))))\n",
    "    trainingData = ratingsData.filter(lambda x: x[0] < 6).values()\n",
    "    validationData = ratingsData.filter(lambda x: x[0] >= 6 and x[0] < 8).values()\n",
    "    testData = ratingsData.filter(lambda x: x[0] >= 8).values()\n",
    "\n",
    "    print(\"training.count()=%d,validation.count()=%d,test.count()=%d\" % (\n",
    "        trainingData.count(), validationData.count(), testData.count()))\n",
    "\n",
    "    moviesData = movies.map(lambda fields: (int(fields[0]), fields[1]))\n",
    "    return trainingData, validationData, testData, moviesData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型参数介绍\n",
    "在构建训练模型时，我们需要用到以下参数：\n",
    "- numBlocks is the number of blocks used to parallelize computation (set to -1 to auto-configure).  \n",
    "- rank is the number of latent factors in the model.  \n",
    "- iterations is the number of iterations to run.  \n",
    "- lambda specifies the regularization parameter in ALS.  \n",
    "- implicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data.  \n",
    "- alpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型，注意，为了更好的调整参数，每个参数都使用了两个值最为备选值，通过\n",
    "# 使模型在用于调参的数据上的误差最小选取参数，这个可以参数表可以自己设置。\n",
    "# train的参数有lambda_是正则化项，blocks表示分区数，设置为-1为使用默认配置\n",
    "# iterations是迭代次数，rank是每一个user或movies的隐含因素的维数。注意，\n",
    "# rank过大或lambda过小都可能导致过拟合，可能导致预测结果偏小\n",
    "def trainModel(trainingData, validationData, testData, iterations=5, lambda_=0.01, blocks=-1):\n",
    "    ranks = [8, 12]\n",
    "    lambdas = [1.0, 10.0]\n",
    "    numIters = [10, 20]\n",
    "    bestModel = None\n",
    "    bestValidationRmse = float(\"inf\")\n",
    "    bestRank = 0\n",
    "    bestLambda = -1.0\n",
    "    bestNumIter = -1\n",
    "\n",
    "    # 计算model在data数据集上的均方误差(Mean Squared Error)\n",
    "    def computeRmse(model, data):\n",
    "        newData = data.map(lambda r: (r[0], r[1]))  # 规整校验数据成（用户,电影），用模型进行评分预测\n",
    "        predictions = model.predictAll(newData).map(lambda r: ((r[0], r[1]), r[2]))  # 预测后返回(用户，电影，评分预测对)\n",
    "        ratesAndPreds = data.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(\n",
    "            predictions)  # 根据(用户，电影)配对连接数据，对应每队(用户，电影)有（评分，预测评分项）\n",
    "        return ratesAndPreds.map(lambda r: (r[1][0] - r[1][1]) ** 2).mean()  # 计算评分和预测评分的均方误差\n",
    "\n",
    "    for rank, lmbda, numIter in itertools.product(ranks, lambdas, numIters):  # 两两组合，形成2*2*2=8组参数循环\n",
    "        model = ALS.train(trainingData, rank, numIter, lmbda, blocks)  # 通过train方法和参数建立ALS训练模型,并通过训练集进行训练\n",
    "        validationRmse = computeRmse(model, validationData)  # 通过校验集，计算训练好的模型进行预测的均方误差\n",
    "        print \"RMSE (validation) = %f for the model trained with \" % validationRmse + \\\n",
    "              \"rank = %d, lambda = %.1f, and numIter = %d.\" % (rank, lmbda, numIter)\n",
    "        if (validationRmse < bestValidationRmse):  # 通过比较校验集历史均方误差，选出最好的模型\n",
    "            bestModel = model\n",
    "            bestValidationRmse = validationRmse\n",
    "            bestRank = rank\n",
    "            bestLambda = lmbda\n",
    "            bestNumIter = numIter\n",
    "\n",
    "    testRmse = computeRmse(bestModel, testData)  # 在得到的最好模型，对测试集进行测试\n",
    "\n",
    "    print \"The best model was trained with rank = %d and lambda = %.1f, \" % (bestRank, bestLambda) \\\n",
    "          + \"and numIter = %d, and its RMSE on the test set is %f.\" % (bestNumIter, testRmse)\n",
    "    return bestModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测。注意使用ALS算法时预测的user_id和movies都必须在训练集中。\n",
    "def predict(model, moviesData, rating, user_id):\n",
    "    rawMoviesData = dict(moviesData.collect())\n",
    "    myRateMovieIdsRDD = rating.filter(lambda x: int(x[0]) == user_id). \\\n",
    "        map(lambda x: x[1]).collect()  # 从测试数据中过滤得到用户数据，再取出该用户的所有电影id\n",
    "    myRateMovieIds = set(myRateMovieIdsRDD)  # 进行去重\n",
    "    candidates = sc.parallelize(\n",
    "        [m for m in rawMoviesData if m not in myRateMovieIds])  # 从最原始的电影数据中过滤用户评过分的电影，得到所有用户未评分的候选电影集，再用于推荐\n",
    "    predictions = model.predictAll(\n",
    "        candidates.map(lambda x: (user_id, x))).collect()  # 建立（userId,movieId）对，通过模型进行预测，得到预测评分。\n",
    "    recommendations = sorted(predictions, key=lambda x: x[2], reverse=True)[:50]  # 对评分进行排序，对前50项进行推荐\n",
    "    print \"Movies recommended for you:\"\n",
    "    for i in xrange(len(recommendations)):  # 输出推荐结果\n",
    "        print (\"%2d: %s\" % (i + 1, rawMoviesData[recommendations[i][1]])).encode('ascii', 'ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('init complete\\xef\\xbc\\x9asc = ', <SparkContext master=local appName=CF>)\n",
      "data.count() = 10000\n",
      "training.count()=5932,validation.count()=2001,test.count()=2067\n",
      "RMSE (validation) = 1.829438 for the model trained with rank = 8, lambda = 1.0, and numIter = 10.\n",
      "RMSE (validation) = 1.828488 for the model trained with rank = 8, lambda = 1.0, and numIter = 20.\n",
      "RMSE (validation) = 14.678594 for the model trained with rank = 8, lambda = 10.0, and numIter = 10.\n",
      "RMSE (validation) = 14.678594 for the model trained with rank = 8, lambda = 10.0, and numIter = 20.\n",
      "RMSE (validation) = 1.830058 for the model trained with rank = 12, lambda = 1.0, and numIter = 10.\n",
      "RMSE (validation) = 1.828441 for the model trained with rank = 12, lambda = 1.0, and numIter = 20.\n",
      "RMSE (validation) = 14.678594 for the model trained with rank = 12, lambda = 10.0, and numIter = 10.\n",
      "RMSE (validation) = 14.678594 for the model trained with rank = 12, lambda = 10.0, and numIter = 20.\n",
      "The best model was trained with rank = 12 and lambda = 1.0, and numIter = 20, and its RMSE on the test set is 1.799925.\n",
      "Movies recommended for you:\n",
      " 1: Love & Sex (2000)\n",
      " 2: 13th Warrior, The (1999)\n",
      " 3: Excalibur (1981)\n",
      " 4: Evil Dead, The (1981)\n",
      " 5: Jonah Who Will Be 25 in the Year 2000 (Jonas qui aura 25 ans en l'an 2000) (1976)\n",
      " 6: Brother from Another Planet, The (1984)\n",
      " 7: Snow Falling on Cedars (1999)\n",
      " 8: Random Hearts (1999)\n",
      " 9: Castle in the Sky (Tenk no shiro Rapyuta) (1986)\n",
      "10: Howl's Moving Castle (Hauru no ugoku shiro) (2004)\n",
      "11: We Were Soldiers (2002)\n",
      "12: Dirty Rotten Scoundrels (1988)\n",
      "13: Tales from the Crypt Presents: Demon Knight (1995)\n",
      "14: Me, Myself and Irene (2000)\n",
      "15: Blood Simple (1984)\n",
      "16: Changing Lanes (2002)\n",
      "17: Miracle on 34th Street (1994)\n",
      "18: Rent (2005)\n",
      "19: Moulin Rouge (2001)\n",
      "20: Belle de jour (1967)\n",
      "21: Ice Storm, The (1997)\n",
      "22: Sin City (2005)\n",
      "23: Mrs. Brown (Her Majesty, Mrs. Brown) (1997)\n",
      "24: How to Lose a Guy in 10 Days (2003)\n",
      "25: Save the Last Dance (2001)\n",
      "26: Spanglish (2004)\n",
      "27: Meet Joe Black (1998)\n",
      "28: Last Kiss, The (2006)\n",
      "29: John Q (2002)\n",
      "30: Perfect Murder, A (1998)\n",
      "31: Polar Express, The (2004)\n",
      "32: Vanya on 42nd Street (1994)\n",
      "33: Age of Innocence, The (1993)\n",
      "34: To Live (Huozhe) (1994)\n",
      "35: Beautiful Thing (1996)\n",
      "36: Shanghai Triad (Yao a yao yao dao waipo qiao) (1995)\n",
      "37: Sweet Smell of Success (1957)\n",
      "38: Jean de Florette (1986)\n",
      "39: Manon of the Spring (Manon des sources) (1986)\n",
      "40: Killer, The (Die xue shuang xiong) (1989)\n",
      "41: Out of Sight (1998)\n",
      "42: Road Warrior, The (Mad Max 2) (1981)\n",
      "43: Tetsuo II: Body Hammer (1992)\n",
      "44: Sandlot, The (1993)\n",
      "45: Mystic River (2003)\n",
      "46: Election (1999)\n",
      "47: Thing, The (1982)\n",
      "48: Angel Heart (1987)\n",
      "49: Donnie Darko (2001)\n",
      "50: Wallace & Gromit: The Best of Aardman Animation (1996)\n"
     ]
    }
   ],
   "source": [
    "# 算法运行主逻辑\n",
    "if __name__ == '__main__':\n",
    "    sc = initSpark()  # 初试化Spark上下文\n",
    "    trainingData, validationData, testData, moviesData = parseData(sc, \"file:///root/notebook/data/ratings.dat\",\n",
    "                                                                   \"file:///root/notebook/data/movies.dat\")  # 解析预处理数据\n",
    "    model = trainModel(trainingData, validationData, testData)  # 得到最佳模型\n",
    "    predict(model, moviesData, testData, 1)  # 根据模型和测试数据给用户进行推荐\n",
    "    sc.stop()  # 终止回收Spark上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果需要存储、读取数据，可运行下面的模型\n",
    "\n",
    "def persistModel(model, sparkContext, modelPath):\n",
    "    model.save(sparkContext, modelPath)\n",
    "\n",
    "\n",
    "def loadModel(sparkContext, modelPath):\n",
    "    from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
    "    return MatrixFactorizationModel.load(sparkContext, modelPath)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import os\n",
    "\n",
    "    rawPath = \"/root/notebook/data/als_model\"\n",
    "    modelPath = \"file://%s\" % rawPath\n",
    "    sc = initSpark()  # 初试化Spark上下文\n",
    "    if os.path.exists(rawPath):\n",
    "        model = loadModel(sc, modelPath)\n",
    "        print(\"load model %s from %s\" % (model, rawPath))\n",
    "    else:\n",
    "        trainingData, validationData, testData, moviesData = parseData(sc, \"file:///root/notebook/data/ratings.dat\",\n",
    "                                                                       \"file:///root/notebook/data/movies.dat\")  # 解析预处理数据\n",
    "        model = trainModel(trainingData, validationData, testData)  # 得到最佳模型\n",
    "    predict(model, moviesData, testData, 1)  # 根据模型和测试数据给用户进行推荐\n",
    "    if not os.path.exists(rawPath):\n",
    "        persistModel(model, sc, modelPath)  # 保存模型，方便下次直接调用\n",
    "        print(\"save model %s to %s\" % (model, rawPath))\n",
    "    sc.stop()  # 终止回收Spark上下文\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
