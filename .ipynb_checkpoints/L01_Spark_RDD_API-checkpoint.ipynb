{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习目标\n",
    "通过本模块，熟悉Spark RDD 常用的API操作，并通过熟悉这些API了解Spark大数据分析的相关思想。\n",
    "\n",
    "## 参考资料\n",
    "1. [开源SparkDemo](https://github.com/baifendian/SparkDemo)\n",
    "2. [Spark官方快速入门文档](http://spark.apache.org/docs/latest/quick-start.htm)\n",
    "3. [Spark官方编程指导文档](http://spark.apache.org/docs/latest/rdd-programming-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\xe5\\x88\\x9d\\xe5\\xa7\\x8b\\xe5\\x8c\\x96\\xe6\\x88\\x90\\xe5\\x8a\\x9f\\xef\\xbc\\x9asc = ', <SparkContext master=local appName=appName>)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time         : 2017/12/26 13:32\n",
    "# @Author       : zenghao\n",
    "\n",
    "from pyspark import SparkContext, SparkConf # 导入相关工具包\n",
    "\n",
    "# 初始化Spark上下文\n",
    "# local为本地调试模式，具体集群方式参照http://spark.apache.org/docs/latest/cluster-overview.html\n",
    "conf = SparkConf().setAppName(\"appName\").setMaster(\"local\") \n",
    "sc = SparkContext(conf=conf)\n",
    "print (\"init complete：sc = \", sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data =', [1, 2, 3, 4, 5])\n",
      "('distData = ', ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:489)\n",
      "DDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).\n",
      "All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n",
      "By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.\n",
      "Local vs. cluster modes\n",
      "The behavior of the above code is undefined, and may not work as intended. To execute jobs, Spark breaks up the processing of RDD operations into tasks, each of which is executed by an executor. Prior to execution, Spark computes the task’s closure. The closure is those variables and methods which must be visible for the executor to perform its computations on the RDD (in this case foreach()). This closure is serialized and sent to each executor.\n",
      "The variables within the closure sent to each executor are now copies and thus, when counter is referenced within the foreach function, it’s no longer the counter on the driver node. There is still a counter in the memory of the driver node but this is no longer visible to the executors! The executors only see the copy from the serialized closure. Thus, the final value of counter will still be zero since all operations on counter were referencing the value within the serialized closure.\n",
      "In local mode, in some circumstances the foreach function will actually execute within the same JVM as the driver and will reference the same original counter, and may actually update it.\n",
      "To ensure well-defined behavior in these sorts of scenarios one should use an Accumulator. Accumulators in Spark are used specifically to provide a mechanism for safely updating a variable when execution is split up across worker nodes in a cluster. The Accumulators section of this guide discusses these in more detail.\n",
      "In general, closures - constructs like loops or locally defined methods, should not be used to mutate some global state. Spark does not define or guarantee the behavior of mutations to objects referenced from outside of closures. Some code that does this may work in local mode, but that’s just by accident and such code will not behave as expected in distributed mode. Use an Accumulator instead if some global aggregation is needed.\n",
      "Printing elements of an RDD\n",
      "Another common idiom is attempting to print out the elements of an RDD using rdd.foreach(println) or rdd.map(println). On a single machine, this will generate the expected output and print all the RDD’s elements. However, in cluster mode, the output to stdout being called by the executors is now writing to the executor’s stdout instead, not the one on the driver, so stdout on the driver won’t show these! To print all elements on the driver, one can use the collect() method to first bring the RDD to the driver node thus: rdd.collect().foreach(println). This can cause the driver to run out of memory, though, because collect() fetches the entire RDD to a single machine; if you only need to print a few elements of the RDD, a safer approach is to use the take(): rdd.take(100).foreach(println).\n"
     ]
    }
   ],
   "source": [
    "# 熟悉文件加载相关操作\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# 将数据加载到Spark 内存\n",
    "# 方式一：通过parallelize并行化\n",
    "distData = sc.parallelize(data)\n",
    "print (\"data =\", data)\n",
    "print (\"distData = \", distData)\n",
    "\n",
    "# 方式二：通过外部文件加载，文件存储位置可涵盖local file system, HDFS, Cassandra, HBase, Amazon S3等\n",
    "# fileData = sc.textFile(\"data/wordcount.txt\")  # 不设置前缀默认从hdfs上加载\n",
    "fileData = sc.textFile(\"file:///root/notebook/data/wordcount.txt\")  # 从本地文件系统读取\n",
    "for line in fileData.collect():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# 熟悉Spark RDD 常用API\n",
    "# python 文档详细参考：http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD\n",
    "# 下面作业需要将data通过特定操作，转换未distData\n",
    "# 实例：\n",
    "data = sc.parallelize([1, 2, 3, 4, 5])\n",
    "myData = None\n",
    "# map(func) \tReturn a new distributed dataset formed by passing each element of the source through a function func.\n",
    "# TODO 将mapData通过map函数转换未mapDistData\n",
    "myData = data.map(lambda x: x * 2).collect()  # 通过collect操作转换为正常的Python数据\n",
    "distData = [2, 4, 6, 8, 10]\n",
    "print(\"task:myData == distData should be True\", myData == distData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8]\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# filter(func) \tReturn a new dataset formed by selecting those elements of the source on which func returns true.\n",
    "data = sc.parallelize([i for i in range(10)])\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data.filter(lambda x: x % 2 == 0).collect()\n",
    "\n",
    "print(myData)\n",
    "distData = [0, 2, 4, 6, 8]\n",
    "print(\"task:myData == distData should be True\", myData == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8, 20, 22, 24, 26, 28]\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# flatMap(func) \tSimilar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).\n",
    "data = sc.parallelize([[i * 10 + j for j in range(5)] for i in range(2)])\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data.flatMap(lambda x: map(lambda y: y * 2, x)).collect()\n",
    "\n",
    "print(myData)\n",
    "distData = [0, 2, 4, 6, 8, 20, 22, 24, 26, 28]\n",
    "print(\"task:myData == distData should be True\", myData == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 7]\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# mapPartitions(func) \tSimilar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator<T> => Iterator<U> when running on an RDD of type T.\n",
    "data = sc.parallelize([1, 2, 3, 4], 2)\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "def f(iterator): yield sum(iterator)\n",
    "myData = data.mapPartitions(f).collect()\n",
    "\n",
    "print(myData)\n",
    "distData = [3, 7]\n",
    "print(\"task:myData == distData should be True\", myData == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HelloWorld\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# union(otherDataset) \tReturn a new dataset that contains the union of the elements in the source dataset and the argument.\n",
    "data1 = sc.parallelize([\"H\", \"e\", \"l\", \"l\", \"o\"])\n",
    "data2 = sc.parallelize([\"W\", \"o\", \"r\", \"l\", \"d\"])\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data1.union(data2).collect()\n",
    "myData = \"\".join(myData)\n",
    "print(myData)\n",
    "distData = \"HelloWorld\"\n",
    "print(\"task:myData == distData should be True\", myData == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', 'l']\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# intersection(otherDataset) \tReturn a new RDD that contains the intersection of elements in the source dataset and the argument.\n",
    "data1 = sc.parallelize([\"H\", \"e\", \"l\", \"l\", \"o\"])\n",
    "data2 = sc.parallelize([\"W\", \"o\", \"r\", \"l\", \"d\"])\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data1.intersection(data2).collect()\n",
    "print(myData)\n",
    "distData = ['o', 'l']\n",
    "print(\"task:myData == distData should be True\", myData == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'o']\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# distinct([numTasks])) \tReturn a new dataset that contains the distinct elements of the source dataset.\n",
    "data = sc.parallelize([\"H\", \"e\", \"l\", \"l\", \"o\"])\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data1.distinct().collect()\n",
    "print(myData)\n",
    "distData = ['H', 'e', 'l', 'o']\n",
    "print(\"task:myData == distData should be True\", myData == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, [0, 2, 4]), (1, [1, 3])]\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# groupByKey([numTasks]) \tWhen called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs.\n",
    "# Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance.\n",
    "# Note: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional numTasks argument to set a different number of tasks.\n",
    "\n",
    "data = sc.parallelize([i for i in range(5)])\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data.groupBy(lambda x: x % 2).collect()\n",
    "myData = sorted([(x, sorted(y)) for (x, y) in myData])\n",
    "\n",
    "print(myData)\n",
    "distData = [(0, [0, 2, 4]), (1, [1, 3])]\n",
    "print(\"task:myData == distData should be True\", myData == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 3), ('c', 9), ('b', 6)]\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# reduceByKey(func, [numTasks]) \tWhen called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.\n",
    "data = sc.parallelize([[(chr(ord('a') + i),i + j) for i in xrange(3)] for j in xrange(3)])\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data.flatMap(lambda x: x).reduceByKey(lambda x, y: x + y).collect()\n",
    "\n",
    "print(myData)\n",
    "distData = [('a', 3), ('c', 9), ('b', 6)]\n",
    "print(\"task:myData == distData should be True\", myData == distData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) \tWhen called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral \"zero\" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.\n",
    "import random\n",
    "data = sc.parallelize([1, 2, 3, 4, 5, 6], 3)\n",
    "\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data.aggregate(0, lambda x, y: max(x, y), lambda x, y: x + y)\n",
    "\n",
    "print(myData)\n",
    "distData = 12\n",
    "print(\"task:myData == distData should be True\", myData == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 6), (2, 4), (2, 7), (3, 1), (4, 5), (6, 2)]\n",
      "[(2, 7), (1, 6), (4, 5), (2, 4), (6, 2), (3, 1)]\n",
      "('task:myData1 == distData1 should be True', True)\n",
      "('task:myData2 == distData2 should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# sortBy(keyfunc, ascending=True, numPartitions=None) Sorts this RDD by the given keyfunc\n",
    "data = sc.parallelize([(3, 1), (2, 4), (6, 2), (1, 6), (4, 5), (2, 7)])\n",
    "myData1 = None\n",
    "myData2 = None\n",
    "\n",
    "# Add your code here\n",
    "myData1 = data.sortBy(lambda x: x[0]).collect()\n",
    "myData2 = data.sortBy(lambda x: x[1], False).collect()\n",
    "print(myData1)\n",
    "print(myData2)\n",
    "\n",
    "distData1 = [(1, 6), (2, 4), (2, 7), (3, 1), (4, 5), (6, 2)]\n",
    "distData2 = [(2, 7), (1, 6), (4, 5), (2, 4), (6, 2), (3, 1)]\n",
    "print(\"task:myData1 == distData1 should be True\", myData1 == distData1)\n",
    "print(\"task:myData2 == distData2 should be True\", myData2 == distData2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-73663530dda4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# join(otherDataset, [numTasks])        When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmyData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# join(otherDataset, [numTasks]) \tWhen called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.\n",
    "data1 = sc.parallelize([(\"a\", 1), (\"b\", 4),(\"b\",1)])\n",
    "data2 = sc.parallelize([(\"a\", 2), (\"a\", 3), (\"b\", 3)])\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data1.join(data2).reduceByKey(lambda x, y: sum(x) + sum(y)).collect()\n",
    "print(myData)\n",
    "\n",
    "distData = [('a', 7), ('b', 11)]\n",
    "print(\"task:myData == distData should be True\", myData == distData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于Spark RDD的常用方法，推荐看pySpark源码部分\n",
    "\n",
    "class RDD(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
    "    Represents an immutable, partitioned collection of elements that can be\n",
    "    operated on in parallel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, jrdd, ctx, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer())):\n",
    "        self._jrdd = jrdd\n",
    "        self.is_cached = False\n",
    "        self.is_checkpointed = False\n",
    "        self.ctx = ctx\n",
    "        self._jrdd_deserializer = jrdd_deserializer\n",
    "        self._id = jrdd.id()\n",
    "        self.partitioner = None\n",
    "\n",
    "    def _pickled(self):\n",
    "        return self._reserialize(AutoBatchedSerializer(PickleSerializer()))\n",
    "\n",
    "    def id(self):\n",
    "        \"\"\"\n",
    "        A unique ID for this RDD (within its SparkContext).\n",
    "        \"\"\"\n",
    "        return self._id\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._jrdd.toString()\n",
    "\n",
    "    def __getnewargs__(self):\n",
    "        # This method is called when attempting to pickle an RDD, which is always an error:\n",
    "        raise Exception(\n",
    "            \"It appears that you are attempting to broadcast an RDD or reference an RDD from an \"\n",
    "            \"action or transformation. RDD transformations and actions can only be invoked by the \"\n",
    "            \"driver, not inside of other transformations; for example, \"\n",
    "            \"rdd1.map(lambda x: rdd2.values.count() * x) is invalid because the values \"\n",
    "            \"transformation and count action cannot be performed inside of the rdd1.map \"\n",
    "            \"transformation. For more information, see SPARK-5063.\"\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def context(self):\n",
    "        \"\"\"\n",
    "        The L{SparkContext} that this RDD was created on.\n",
    "        \"\"\"\n",
    "        return self.ctx\n",
    "\n",
    "    def cache(self):\n",
    "        \"\"\"\n",
    "        Persist this RDD with the default storage level (C{MEMORY_ONLY}).\n",
    "        \"\"\"\n",
    "        self.is_cached = True\n",
    "        self.persist(StorageLevel.MEMORY_ONLY)\n",
    "        return self\n",
    "\n",
    "    def persist(self, storageLevel=StorageLevel.MEMORY_ONLY):\n",
    "        \"\"\"\n",
    "        Set this RDD's storage level to persist its values across operations\n",
    "        after the first time it is computed. This can only be used to assign\n",
    "        a new storage level if the RDD does not have a storage level set yet.\n",
    "        If no storage level is specified defaults to (C{MEMORY_ONLY}).\n",
    "\n",
    "        >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
    "        >>> rdd.persist().is_cached\n",
    "        True\n",
    "        \"\"\"\n",
    "        self.is_cached = True\n",
    "        javaStorageLevel = self.ctx._getJavaStorageLevel(storageLevel)\n",
    "        self._jrdd.persist(javaStorageLevel)\n",
    "        return self\n",
    "\n",
    "    def unpersist(self):\n",
    "        \"\"\"\n",
    "        Mark the RDD as non-persistent, and remove all blocks for it from\n",
    "        memory and disk.\n",
    "        \"\"\"\n",
    "        self.is_cached = False\n",
    "        self._jrdd.unpersist()\n",
    "        return self\n",
    "\n",
    "    def checkpoint(self):\n",
    "        \"\"\"\n",
    "        Mark this RDD for checkpointing. It will be saved to a file inside the\n",
    "        checkpoint directory set with L{SparkContext.setCheckpointDir()} and\n",
    "        all references to its parent RDDs will be removed. This function must\n",
    "        be called before any job has been executed on this RDD. It is strongly\n",
    "        recommended that this RDD is persisted in memory, otherwise saving it\n",
    "        on a file will require recomputation.\n",
    "        \"\"\"\n",
    "        self.is_checkpointed = True\n",
    "        self._jrdd.rdd().checkpoint()\n",
    "\n",
    "    def isCheckpointed(self):\n",
    "        \"\"\"\n",
    "        Return whether this RDD is checkpointed and materialized, either reliably or locally.\n",
    "        \"\"\"\n",
    "        return self._jrdd.rdd().isCheckpointed()\n",
    "\n",
    "    def localCheckpoint(self):\n",
    "        \"\"\"\n",
    "        Mark this RDD for local checkpointing using Spark's existing caching layer.\n",
    "\n",
    "        This method is for users who wish to truncate RDD lineages while skipping the expensive\n",
    "        step of replicating the materialized data in a reliable distributed file system. This is\n",
    "        useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n",
    "\n",
    "        Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n",
    "        data is written to ephemeral local storage in the executors instead of to a reliable,\n",
    "        fault-tolerant storage. The effect is that if an executor fails during the computation,\n",
    "        the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n",
    "\n",
    "        This is NOT safe to use with dynamic allocation, which removes executors along\n",
    "        with their cached blocks. If you must use both features, you are advised to set\n",
    "        L{spark.dynamicAllocation.cachedExecutorIdleTimeout} to a high value.\n",
    "\n",
    "        The checkpoint directory set through L{SparkContext.setCheckpointDir()} is not used.\n",
    "        \"\"\"\n",
    "        self._jrdd.rdd().localCheckpoint()\n",
    "\n",
    "    def isLocallyCheckpointed(self):\n",
    "        \"\"\"\n",
    "        Return whether this RDD is marked for local checkpointing.\n",
    "\n",
    "        Exposed for testing.\n",
    "        \"\"\"\n",
    "        return self._jrdd.rdd().isLocallyCheckpointed()\n",
    "\n",
    "    def getCheckpointFile(self):\n",
    "        \"\"\"\n",
    "        Gets the name of the file to which this RDD was checkpointed\n",
    "\n",
    "        Not defined if RDD is checkpointed locally.\n",
    "        \"\"\"\n",
    "        checkpointFile = self._jrdd.rdd().getCheckpointFile()\n",
    "        if checkpointFile.isDefined():\n",
    "            return checkpointFile.get()\n",
    "\n",
    "    def map(self, f, preservesPartitioning=False):\n",
    "        \"\"\"\n",
    "        Return a new RDD by applying a function to each element of this RDD.\n",
    "\n",
    "        >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
    "        >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
    "        [('a', 1), ('b', 1), ('c', 1)]\n",
    "        \"\"\"\n",
    "        def func(_, iterator):\n",
    "            return map(f, iterator)\n",
    "        return self.mapPartitionsWithIndex(func, preservesPartitioning)\n",
    "\n",
    "    def flatMap(self, f, preservesPartitioning=False):\n",
    "        \"\"\"\n",
    "        Return a new RDD by first applying a function to all elements of this\n",
    "        RDD, and then flattening the results.\n",
    "\n",
    "        >>> rdd = sc.parallelize([2, 3, 4])\n",
    "        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
    "        [1, 1, 1, 2, 2, 3]\n",
    "        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
    "        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
    "        \"\"\"\n",
    "        def func(s, iterator):\n",
    "            return chain.from_iterable(map(f, iterator))\n",
    "        return self.mapPartitionsWithIndex(func, preservesPartitioning)\n",
    "\n",
    "    def mapPartitions(self, f, preservesPartitioning=False):\n",
    "        \"\"\"\n",
    "        Return a new RDD by applying a function to each partition of this RDD.\n",
    "\n",
    "        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
    "        >>> def f(iterator): yield sum(iterator)\n",
    "        >>> rdd.mapPartitions(f).collect()\n",
    "        [3, 7]\n",
    "        \"\"\"\n",
    "        def func(s, iterator):\n",
    "            return f(iterator)\n",
    "        return self.mapPartitionsWithIndex(func, preservesPartitioning)\n",
    "\n",
    "    def mapPartitionsWithIndex(self, f, preservesPartitioning=False):\n",
    "        \"\"\"\n",
    "        Return a new RDD by applying a function to each partition of this RDD,\n",
    "        while tracking the index of the original partition.\n",
    "\n",
    "        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
    "        >>> def f(splitIndex, iterator): yield splitIndex\n",
    "        >>> rdd.mapPartitionsWithIndex(f).sum()\n",
    "        6\n",
    "        \"\"\"\n",
    "        return PipelinedRDD(self, f, preservesPartitioning)\n",
    "\n",
    "    def mapPartitionsWithSplit(self, f, preservesPartitioning=False):\n",
    "        \"\"\"\n",
    "        Deprecated: use mapPartitionsWithIndex instead.\n",
    "\n",
    "        Return a new RDD by applying a function to each partition of this RDD,\n",
    "        while tracking the index of the original partition.\n",
    "\n",
    "        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
    "        >>> def f(splitIndex, iterator): yield splitIndex\n",
    "        >>> rdd.mapPartitionsWithSplit(f).sum()\n",
    "        6\n",
    "        \"\"\"\n",
    "        warnings.warn(\"mapPartitionsWithSplit is deprecated; \"\n",
    "                      \"use mapPartitionsWithIndex instead\", DeprecationWarning, stacklevel=2)\n",
    "        return self.mapPartitionsWithIndex(f, preservesPartitioning)\n",
    "\n",
    "    def getNumPartitions(self):\n",
    "        \"\"\"\n",
    "        Returns the number of partitions in RDD\n",
    "\n",
    "        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
    "        >>> rdd.getNumPartitions()\n",
    "        2\n",
    "        \"\"\"\n",
    "        return self._jrdd.partitions().size()\n",
    "\n",
    "    def filter(self, f):\n",
    "        \"\"\"\n",
    "        Return a new RDD containing only the elements that satisfy a predicate.\n",
    "\n",
    "        >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "        >>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
    "        [2, 4]\n",
    "        \"\"\"\n",
    "        def func(iterator):\n",
    "            return filter(f, iterator)\n",
    "        return self.mapPartitions(func, True)\n",
    "\n",
    "    def distinct(self, numPartitions=None):\n",
    "        \"\"\"\n",
    "        Return a new RDD containing the distinct elements in this RDD.\n",
    "\n",
    "        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
    "        [1, 2, 3]\n",
    "        \"\"\"\n",
    "        return self.map(lambda x: (x, None)) \\\n",
    "                   .reduceByKey(lambda x, _: x, numPartitions) \\\n",
    "                   .map(lambda x: x[0])\n",
    "\n",
    "    def sample(self, withReplacement, fraction, seed=None):\n",
    "        \"\"\"\n",
    "        Return a sampled subset of this RDD.\n",
    "\n",
    "        :param withReplacement: can elements be sampled multiple times (replaced when sampled out)\n",
    "        :param fraction: expected size of the sample as a fraction of this RDD's size\n",
    "            without replacement: probability that each element is chosen; fraction must be [0, 1]\n",
    "            with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
    "        :param seed: seed for the random number generator\n",
    "\n",
    "        .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n",
    "            count of the given :class:`DataFrame`.\n",
    "\n",
    "        >>> rdd = sc.parallelize(range(100), 4)\n",
    "        >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n",
    "        True\n",
    "        \"\"\"\n",
    "        assert fraction >= 0.0, \"Negative fraction value: %s\" % fraction\n",
    "        return self.mapPartitionsWithIndex(RDDSampler(withReplacement, fraction, seed).func, True)\n",
    "\n",
    "    def randomSplit(self, weights, seed=None):\n",
    "        \"\"\"\n",
    "        Randomly splits this RDD with the provided weights.\n",
    "\n",
    "        :param weights: weights for splits, will be normalized if they don't sum to 1\n",
    "        :param seed: random seed\n",
    "        :return: split RDDs in a list\n",
    "\n",
    "        >>> rdd = sc.parallelize(range(500), 1)\n",
    "        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
    "        >>> len(rdd1.collect() + rdd2.collect())\n",
    "        500\n",
    "        >>> 150 < rdd1.count() < 250\n",
    "        True\n",
    "        >>> 250 < rdd2.count() < 350\n",
    "        True\n",
    "        \"\"\"\n",
    "        s = float(sum(weights))\n",
    "        cweights = [0.0]\n",
    "        for w in weights:\n",
    "            cweights.append(cweights[-1] + w / s)\n",
    "        if seed is None:\n",
    "            seed = random.randint(0, 2 ** 32 - 1)\n",
    "        return [self.mapPartitionsWithIndex(RDDRangeSampler(lb, ub, seed).func, True)\n",
    "                for lb, ub in zip(cweights, cweights[1:])]\n",
    "\n",
    "    # this is ported from scala/spark/RDD.scala\n",
    "    def takeSample(self, withReplacement, num, seed=None):\n",
    "        \"\"\"\n",
    "        Return a fixed-size sampled subset of this RDD.\n",
    "\n",
    "        .. note:: This method should only be used if the resulting array is expected\n",
    "            to be small, as all the data is loaded into the driver's memory.\n",
    "\n",
    "        >>> rdd = sc.parallelize(range(0, 10))\n",
    "        >>> len(rdd.takeSample(True, 20, 1))\n",
    "        20\n",
    "        >>> len(rdd.takeSample(False, 5, 2))\n",
    "        5\n",
    "        >>> len(rdd.takeSample(False, 15, 3))\n",
    "        10\n",
    "        \"\"\"\n",
    "        numStDev = 10.0\n",
    "\n",
    "        if num < 0:\n",
    "            raise ValueError(\"Sample size cannot be negative.\")\n",
    "        elif num == 0:\n",
    "            return []\n",
    "\n",
    "        initialCount = self.count()\n",
    "        if initialCount == 0:\n",
    "            return []\n",
    "\n",
    "        rand = random.Random(seed)\n",
    "\n",
    "        if (not withReplacement) and num >= initialCount:\n",
    "            # shuffle current RDD and return\n",
    "            samples = self.collect()\n",
    "            rand.shuffle(samples)\n",
    "            return samples\n",
    "\n",
    "        maxSampleSize = sys.maxsize - int(numStDev * sqrt(sys.maxsize))\n",
    "        if num > maxSampleSize:\n",
    "            raise ValueError(\n",
    "                \"Sample size cannot be greater than %d.\" % maxSampleSize)\n",
    "\n",
    "        fraction = RDD._computeFractionForSampleSize(\n",
    "            num, initialCount, withReplacement)\n",
    "        samples = self.sample(withReplacement, fraction, seed).collect()\n",
    "\n",
    "        # If the first sample didn't turn out large enough, keep trying to take samples;\n",
    "        # this shouldn't happen often because we use a big multiplier for their initial size.\n",
    "        # See: scala/spark/RDD.scala\n",
    "        while len(samples) < num:\n",
    "            # TODO: add log warning for when more than one iteration was run\n",
    "            seed = rand.randint(0, sys.maxsize)\n",
    "            samples = self.sample(withReplacement, fraction, seed).collect()\n",
    "\n",
    "        rand.shuffle(samples)\n",
    "\n",
    "        return samples[0:num]\n",
    "\n",
    "    @staticmethod\n",
    "    def _computeFractionForSampleSize(sampleSizeLowerBound, total, withReplacement):\n",
    "        \"\"\"\n",
    "        Returns a sampling rate that guarantees a sample of\n",
    "        size >= sampleSizeLowerBound 99.99% of the time.\n",
    "\n",
    "        How the sampling rate is determined:\n",
    "        Let p = num / total, where num is the sample size and total is the\n",
    "        total number of data points in the RDD. We're trying to compute\n",
    "        q > p such that\n",
    "          - when sampling with replacement, we're drawing each data point\n",
    "            with prob_i ~ Pois(q), where we want to guarantee\n",
    "            Pr[s < num] < 0.0001 for s = sum(prob_i for i from 0 to\n",
    "            total), i.e. the failure rate of not having a sufficiently large\n",
    "            sample < 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient\n",
    "            to guarantee 0.9999 success rate for num > 12, but we need a\n",
    "            slightly larger q (9 empirically determined).\n",
    "          - when sampling without replacement, we're drawing each data point\n",
    "            with prob_i ~ Binomial(total, fraction) and our choice of q\n",
    "            guarantees 1-delta, or 0.9999 success rate, where success rate is\n",
    "            defined the same as in sampling with replacement.\n",
    "        \"\"\"\n",
    "        fraction = float(sampleSizeLowerBound) / total\n",
    "        if withReplacement:\n",
    "            numStDev = 5\n",
    "            if (sampleSizeLowerBound < 12):\n",
    "                numStDev = 9\n",
    "            return fraction + numStDev * sqrt(fraction / total)\n",
    "        else:\n",
    "            delta = 0.00005\n",
    "            gamma = - log(delta) / total\n",
    "            return min(1, fraction + gamma + sqrt(gamma * gamma + 2 * gamma * fraction))\n",
    "\n",
    "    def union(self, other):\n",
    "        \"\"\"\n",
    "        Return the union of this RDD and another one.\n",
    "\n",
    "        >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
    "        >>> rdd.union(rdd).collect()\n",
    "        [1, 1, 2, 3, 1, 1, 2, 3]\n",
    "        \"\"\"\n",
    "        if self._jrdd_deserializer == other._jrdd_deserializer:\n",
    "            rdd = RDD(self._jrdd.union(other._jrdd), self.ctx,\n",
    "                      self._jrdd_deserializer)\n",
    "        else:\n",
    "            # These RDDs contain data in different serialized formats, so we\n",
    "            # must normalize them to the default serializer.\n",
    "            self_copy = self._reserialize()\n",
    "            other_copy = other._reserialize()\n",
    "            rdd = RDD(self_copy._jrdd.union(other_copy._jrdd), self.ctx,\n",
    "                      self.ctx.serializer)\n",
    "        if (self.partitioner == other.partitioner and\n",
    "                self.getNumPartitions() == rdd.getNumPartitions()):\n",
    "            rdd.partitioner = self.partitioner\n",
    "        return rdd\n",
    "\n",
    "    def intersection(self, other):\n",
    "        \"\"\"\n",
    "        Return the intersection of this RDD and another one. The output will\n",
    "        not contain any duplicate elements, even if the input RDDs did.\n",
    "\n",
    "        .. note:: This method performs a shuffle internally.\n",
    "\n",
    "        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
    "        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
    "        >>> rdd1.intersection(rdd2).collect()\n",
    "        [1, 2, 3]\n",
    "        \"\"\"\n",
    "        return self.map(lambda v: (v, None)) \\\n",
    "            .cogroup(other.map(lambda v: (v, None))) \\\n",
    "            .filter(lambda k_vs: all(k_vs[1])) \\\n",
    "            .keys()\n",
    "\n",
    "    def _reserialize(self, serializer=None):\n",
    "        serializer = serializer or self.ctx.serializer\n",
    "        if self._jrdd_deserializer != serializer:\n",
    "            self = self.map(lambda x: x, preservesPartitioning=True)\n",
    "            self._jrdd_deserializer = serializer\n",
    "        return self\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Return the union of this RDD and another one.\n",
    "\n",
    "        >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
    "        >>> (rdd + rdd).collect()\n",
    "        [1, 1, 2, 3, 1, 1, 2, 3]\n",
    "        \"\"\"\n",
    "        if not isinstance(other, RDD):\n",
    "            raise TypeError\n",
    "        return self.union(other)\n",
    "\n",
    "    def repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=portable_hash,\n",
    "                                           ascending=True, keyfunc=lambda x: x):\n",
    "        \"\"\"\n",
    "        Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
    "        sort records by their keys.\n",
    "\n",
    "        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
    "        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, 2)\n",
    "        >>> rdd2.glom().collect()\n",
    "        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n",
    "        \"\"\"\n",
    "        if numPartitions is None:\n",
    "            numPartitions = self._defaultReducePartitions()\n",
    "\n",
    "        memory = _parse_memory(self.ctx._conf.get(\"spark.python.worker.memory\", \"512m\"))\n",
    "        serializer = self._jrdd_deserializer\n",
    "\n",
    "        def sortPartition(iterator):\n",
    "            sort = ExternalSorter(memory * 0.9, serializer).sorted\n",
    "            return iter(sort(iterator, key=lambda k_v: keyfunc(k_v[0]), reverse=(not ascending)))\n",
    "\n",
    "        return self.partitionBy(numPartitions, partitionFunc).mapPartitions(sortPartition, True)\n",
    "\n",
    "    def sortByKey(self, ascending=True, numPartitions=None, keyfunc=lambda x: x):\n",
    "        \"\"\"\n",
    "        Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
    "        # noqa\n",
    "\n",
    "        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "        >>> sc.parallelize(tmp).sortByKey().first()\n",
    "        ('1', 3)\n",
    "        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
    "        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
    "        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
    "        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
    "        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "        >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "        >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
    "        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n",
    "        \"\"\"\n",
    "        if numPartitions is None:\n",
    "            numPartitions = self._defaultReducePartitions()\n",
    "\n",
    "        memory = self._memory_limit()\n",
    "        serializer = self._jrdd_deserializer\n",
    "\n",
    "        def sortPartition(iterator):\n",
    "            sort = ExternalSorter(memory * 0.9, serializer).sorted\n",
    "            return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))\n",
    "\n",
    "        if numPartitions == 1:\n",
    "            if self.getNumPartitions() > 1:\n",
    "                self = self.coalesce(1)\n",
    "            return self.mapPartitions(sortPartition, True)\n",
    "\n",
    "        # first compute the boundary of each part via sampling: we want to partition\n",
    "        # the key-space into bins such that the bins have roughly the same\n",
    "        # number of (key, value) pairs falling into them\n",
    "        rddSize = self.count()\n",
    "        if not rddSize:\n",
    "            return self  # empty RDD\n",
    "        maxSampleSize = numPartitions * 20.0  # constant from Spark's RangePartitioner\n",
    "        fraction = min(maxSampleSize / max(rddSize, 1), 1.0)\n",
    "        samples = self.sample(False, fraction, 1).map(lambda kv: kv[0]).collect()\n",
    "        samples = sorted(samples, key=keyfunc)\n",
    "\n",
    "        # we have numPartitions many parts but one of the them has\n",
    "        # an implicit boundary\n",
    "        bounds = [samples[int(len(samples) * (i + 1) / numPartitions)]\n",
    "                  for i in range(0, numPartitions - 1)]\n",
    "\n",
    "        def rangePartitioner(k):\n",
    "            p = bisect.bisect_left(bounds, keyfunc(k))\n",
    "            if ascending:\n",
    "                return p\n",
    "            else:\n",
    "                return numPartitions - 1 - p\n",
    "\n",
    "        return self.partitionBy(numPartitions, rangePartitioner).mapPartitions(sortPartition, True)\n",
    "\n",
    "    def sortBy(self, keyfunc, ascending=True, numPartitions=None):\n",
    "        \"\"\"\n",
    "        Sorts this RDD by the given keyfunc\n",
    "\n",
    "        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
    "        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
    "        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
    "        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "        \"\"\"\n",
    "        return self.keyBy(keyfunc).sortByKey(ascending, numPartitions).values()\n",
    "\n",
    "    def glom(self):\n",
    "        \"\"\"\n",
    "        Return an RDD created by coalescing all elements within each partition\n",
    "        into a list.\n",
    "\n",
    "        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
    "        >>> sorted(rdd.glom().collect())\n",
    "        [[1, 2], [3, 4]]\n",
    "        \"\"\"\n",
    "        def func(iterator):\n",
    "            yield list(iterator)\n",
    "        return self.mapPartitions(func)\n",
    "\n",
    "    def cartesian(self, other):\n",
    "        \"\"\"\n",
    "        Return the Cartesian product of this RDD and another one, that is, the\n",
    "        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and\n",
    "        C{b} is in C{other}.\n",
    "\n",
    "        >>> rdd = sc.parallelize([1, 2])\n",
    "        >>> sorted(rdd.cartesian(rdd).collect())\n",
    "        [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
    "        \"\"\"\n",
    "        # Due to batching, we can't use the Java cartesian method.\n",
    "        deserializer = CartesianDeserializer(self._jrdd_deserializer,\n",
    "                                             other._jrdd_deserializer)\n",
    "        return RDD(self._jrdd.cartesian(other._jrdd), self.ctx, deserializer)\n",
    "\n",
    "    def groupBy(self, f, numPartitions=None, partitionFunc=portable_hash):\n",
    "        \"\"\"\n",
    "        Return an RDD of grouped items.\n",
    "\n",
    "        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "        >>> result = rdd.groupBy(lambda x: x % 2).collect()\n",
    "        >>> sorted([(x, sorted(y)) for (x, y) in result])\n",
    "        [(0, [2, 8]), (1, [1, 1, 3, 5])]\n",
    "        \"\"\"\n",
    "        return self.map(lambda x: (f(x), x)).groupByKey(numPartitions, partitionFunc)\n",
    "\n",
    "    @ignore_unicode_prefix\n",
    "    def pipe(self, command, env=None, checkCode=False):\n",
    "        \"\"\"\n",
    "        Return an RDD created by piping elements to a forked external process.\n",
    "\n",
    "        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n",
    "        [u'1', u'2', u'', u'3']\n",
    "\n",
    "        :param checkCode: whether or not to check the return value of the shell command.\n",
    "        \"\"\"\n",
    "        if env is None:\n",
    "            env = dict()\n",
    "\n",
    "        def func(iterator):\n",
    "            pipe = Popen(\n",
    "                shlex.split(command), env=env, stdin=PIPE, stdout=PIPE)\n",
    "\n",
    "            def pipe_objs(out):\n",
    "                for obj in iterator:\n",
    "                    s = str(obj).rstrip('\\n') + '\\n'\n",
    "                    out.write(s.encode('utf-8'))\n",
    "                out.close()\n",
    "            Thread(target=pipe_objs, args=[pipe.stdin]).start()\n",
    "\n",
    "            def check_return_code():\n",
    "                pipe.wait()\n",
    "                if checkCode and pipe.returncode:\n",
    "                    raise Exception(\"Pipe function `%s' exited \"\n",
    "                                    \"with error code %d\" % (command, pipe.returncode))\n",
    "                else:\n",
    "                    for i in range(0):\n",
    "                        yield i\n",
    "            return (x.rstrip(b'\\n').decode('utf-8') for x in\n",
    "                    chain(iter(pipe.stdout.readline, b''), check_return_code()))\n",
    "        return self.mapPartitions(func)\n",
    "\n",
    "    def foreach(self, f):\n",
    "        \"\"\"\n",
    "        Applies a function to all elements of this RDD.\n",
    "\n",
    "        >>> def f(x): print(x)\n",
    "        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
    "        \"\"\"\n",
    "        def processPartition(iterator):\n",
    "            for x in iterator:\n",
    "                f(x)\n",
    "            return iter([])\n",
    "        self.mapPartitions(processPartition).count()  # Force evaluation\n",
    "\n",
    "    def foreachPartition(self, f):\n",
    "        \"\"\"\n",
    "        Applies a function to each partition of this RDD.\n",
    "\n",
    "        >>> def f(iterator):\n",
    "        ...     for x in iterator:\n",
    "        ...          print(x)\n",
    "        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n",
    "        \"\"\"\n",
    "        def func(it):\n",
    "            r = f(it)\n",
    "            try:\n",
    "                return iter(r)\n",
    "            except TypeError:\n",
    "                return iter([])\n",
    "        self.mapPartitions(func).count()  # Force evaluation\n",
    "\n",
    "    def collect(self):\n",
    "        \"\"\"\n",
    "        Return a list that contains all of the elements in this RDD.\n",
    "\n",
    "        .. note:: This method should only be used if the resulting array is expected\n",
    "            to be small, as all the data is loaded into the driver's memory.\n",
    "        \"\"\"\n",
    "        with SCCallSiteSync(self.context) as css:\n",
    "            port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n",
    "        return list(_load_from_socket(port, self._jrdd_deserializer))\n",
    "\n",
    "    def reduce(self, f):\n",
    "        \"\"\"\n",
    "        Reduces the elements of this RDD using the specified commutative and\n",
    "        associative binary operator. Currently reduces partitions locally.\n",
    "\n",
    "        >>> from operator import add\n",
    "        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
    "        15\n",
    "        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
    "        10\n",
    "        >>> sc.parallelize([]).reduce(add)\n",
    "        Traceback (most recent call last):\n",
    "            ...\n",
    "        ValueError: Can not reduce() empty RDD\n",
    "        \"\"\"\n",
    "        def func(iterator):\n",
    "            iterator = iter(iterator)\n",
    "            try:\n",
    "                initial = next(iterator)\n",
    "            except StopIteration:\n",
    "                return\n",
    "            yield reduce(f, iterator, initial)\n",
    "\n",
    "        vals = self.mapPartitions(func).collect()\n",
    "        if vals:\n",
    "            return reduce(f, vals)\n",
    "        raise ValueError(\"Can not reduce() empty RDD\")\n",
    "\n",
    "    def treeReduce(self, f, depth=2):\n",
    "        \"\"\"\n",
    "        Reduces the elements of this RDD in a multi-level tree pattern.\n",
    "\n",
    "        :param depth: suggested depth of the tree (default: 2)\n",
    "\n",
    "        >>> add = lambda x, y: x + y\n",
    "        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
    "        >>> rdd.treeReduce(add)\n",
    "        -5\n",
    "        >>> rdd.treeReduce(add, 1)\n",
    "        -5\n",
    "        >>> rdd.treeReduce(add, 2)\n",
    "        -5\n",
    "        >>> rdd.treeReduce(add, 5)\n",
    "        -5\n",
    "        >>> rdd.treeReduce(add, 10)\n",
    "        -5\n",
    "        \"\"\"\n",
    "        if depth < 1:\n",
    "            raise ValueError(\"Depth cannot be smaller than 1 but got %d.\" % depth)\n",
    "\n",
    "        zeroValue = None, True  # Use the second entry to indicate whether this is a dummy value.\n",
    "\n",
    "        def op(x, y):\n",
    "            if x[1]:\n",
    "                return y\n",
    "            elif y[1]:\n",
    "                return x\n",
    "            else:\n",
    "                return f(x[0], y[0]), False\n",
    "\n",
    "        reduced = self.map(lambda x: (x, False)).treeAggregate(zeroValue, op, op, depth)\n",
    "        if reduced[1]:\n",
    "            raise ValueError(\"Cannot reduce empty RDD.\")\n",
    "        return reduced[0]\n",
    "\n",
    "    def fold(self, zeroValue, op):\n",
    "        \"\"\"\n",
    "        Aggregate the elements of each partition, and then the results for all\n",
    "        the partitions, using a given associative function and a neutral \"zero value.\"\n",
    "\n",
    "        The function C{op(t1, t2)} is allowed to modify C{t1} and return it\n",
    "        as its result value to avoid object allocation; however, it should not\n",
    "        modify C{t2}.\n",
    "\n",
    "        This behaves somewhat differently from fold operations implemented\n",
    "        for non-distributed collections in functional languages like Scala.\n",
    "        This fold operation may be applied to partitions individually, and then\n",
    "        fold those results into the final result, rather than apply the fold\n",
    "        to each element sequentially in some defined ordering. For functions\n",
    "        that are not commutative, the result may differ from that of a fold\n",
    "        applied to a non-distributed collection.\n",
    "\n",
    "        >>> from operator import add\n",
    "        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
    "        15\n",
    "        \"\"\"\n",
    "        def func(iterator):\n",
    "            acc = zeroValue\n",
    "            for obj in iterator:\n",
    "                acc = op(acc, obj)\n",
    "            yield acc\n",
    "        # collecting result of mapPartitions here ensures that the copy of\n",
    "        # zeroValue provided to each partition is unique from the one provided\n",
    "        # to the final reduce call\n",
    "        vals = self.mapPartitions(func).collect()\n",
    "        return reduce(op, vals, zeroValue)\n",
    "\n",
    "    def aggregate(self, zeroValue, seqOp, combOp):\n",
    "        \"\"\"\n",
    "        Aggregate the elements of each partition, and then the results for all\n",
    "        the partitions, using a given combine functions and a neutral \"zero\n",
    "        value.\"\n",
    "\n",
    "        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it\n",
    "        as its result value to avoid object allocation; however, it should not\n",
    "        modify C{t2}.\n",
    "\n",
    "        The first function (seqOp) can return a different result type, U, than\n",
    "        the type of this RDD. Thus, we need one operation for merging a T into\n",
    "        an U and one operation for merging two U\n",
    "\n",
    "        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
    "        (10, 4)\n",
    "        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
    "        (0, 0)\n",
    "        \"\"\"\n",
    "        def func(iterator):\n",
    "            acc = zeroValue\n",
    "            for obj in iterator:\n",
    "                acc = seqOp(acc, obj)\n",
    "            yield acc\n",
    "        # collecting result of mapPartitions here ensures that the copy of\n",
    "        # zeroValue provided to each partition is unique from the one provided\n",
    "        # to the final reduce call\n",
    "        vals = self.mapPartitions(func).collect()\n",
    "        return reduce(combOp, vals, zeroValue)\n",
    "\n",
    "    def treeAggregate(self, zeroValue, seqOp, combOp, depth=2):\n",
    "        \"\"\"\n",
    "        Aggregates the elements of this RDD in a multi-level tree\n",
    "        pattern.\n",
    "\n",
    "        :param depth: suggested depth of the tree (default: 2)\n",
    "\n",
    "        >>> add = lambda x, y: x + y\n",
    "        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
    "        >>> rdd.treeAggregate(0, add, add)\n",
    "        -5\n",
    "        >>> rdd.treeAggregate(0, add, add, 1)\n",
    "        -5\n",
    "        >>> rdd.treeAggregate(0, add, add, 2)\n",
    "        -5\n",
    "        >>> rdd.treeAggregate(0, add, add, 5)\n",
    "        -5\n",
    "        >>> rdd.treeAggregate(0, add, add, 10)\n",
    "        -5\n",
    "        \"\"\"\n",
    "        if depth < 1:\n",
    "            raise ValueError(\"Depth cannot be smaller than 1 but got %d.\" % depth)\n",
    "\n",
    "        if self.getNumPartitions() == 0:\n",
    "            return zeroValue\n",
    "\n",
    "        def aggregatePartition(iterator):\n",
    "            acc = zeroValue\n",
    "            for obj in iterator:\n",
    "                acc = seqOp(acc, obj)\n",
    "            yield acc\n",
    "\n",
    "        partiallyAggregated = self.mapPartitions(aggregatePartition)\n",
    "        numPartitions = partiallyAggregated.getNumPartitions()\n",
    "        scale = max(int(ceil(pow(numPartitions, 1.0 / depth))), 2)\n",
    "        # If creating an extra level doesn't help reduce the wall-clock time, we stop the tree\n",
    "        # aggregation.\n",
    "        while numPartitions > scale + numPartitions / scale:\n",
    "            numPartitions /= scale\n",
    "            curNumPartitions = int(numPartitions)\n",
    "\n",
    "            def mapPartition(i, iterator):\n",
    "                for obj in iterator:\n",
    "                    yield (i % curNumPartitions, obj)\n",
    "\n",
    "            partiallyAggregated = partiallyAggregated \\\n",
    "                .mapPartitionsWithIndex(mapPartition) \\\n",
    "                .reduceByKey(combOp, curNumPartitions) \\\n",
    "                .values()\n",
    "\n",
    "        return partiallyAggregated.reduce(combOp)\n",
    "\n",
    "    def max(self, key=None):\n",
    "        \"\"\"\n",
    "        Find the maximum item in this RDD.\n",
    "\n",
    "        :param key: A function used to generate key for comparing\n",
    "\n",
    "        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n",
    "        >>> rdd.max()\n",
    "        43.0\n",
    "        >>> rdd.max(key=str)\n",
    "        5.0\n",
    "        \"\"\"\n",
    "        if key is None:\n",
    "            return self.reduce(max)\n",
    "        return self.reduce(lambda a, b: max(a, b, key=key))\n",
    "\n",
    "    def min(self, key=None):\n",
    "        \"\"\"\n",
    "        Find the minimum item in this RDD.\n",
    "\n",
    "        :param key: A function used to generate key for comparing\n",
    "\n",
    "        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n",
    "        >>> rdd.min()\n",
    "        2.0\n",
    "        >>> rdd.min(key=str)\n",
    "        10.0\n",
    "        \"\"\"\n",
    "        if key is None:\n",
    "            return self.reduce(min)\n",
    "        return self.reduce(lambda a, b: min(a, b, key=key))\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"\n",
    "        Add up the elements in this RDD.\n",
    "\n",
    "        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
    "        6.0\n",
    "        \"\"\"\n",
    "        return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n",
    "\n",
    "    def count(self):\n",
    "        \"\"\"\n",
    "        Return the number of elements in this RDD.\n",
    "\n",
    "        >>> sc.parallelize([2, 3, 4]).count()\n",
    "        3\n",
    "        \"\"\"\n",
    "        return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n",
    "\n",
    "    def stats(self):\n",
    "        \"\"\"\n",
    "        Return a L{StatCounter} object that captures the mean, variance\n",
    "        and count of the RDD's elements in one operation.\n",
    "        \"\"\"\n",
    "        def redFunc(left_counter, right_counter):\n",
    "            return left_counter.mergeStats(right_counter)\n",
    "\n",
    "        return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)\n",
    "\n",
    "    def histogram(self, buckets):\n",
    "        \"\"\"\n",
    "        Compute a histogram using the provided buckets. The buckets\n",
    "        are all open to the right except for the last which is closed.\n",
    "        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n",
    "        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n",
    "        and 50 we would have a histogram of 1,0,1.\n",
    "\n",
    "        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n",
    "        this can be switched from an O(log n) inseration to O(1) per\n",
    "        element (where n is the number of buckets).\n",
    "\n",
    "        Buckets must be sorted, not contain any duplicates, and have\n",
    "        at least two elements.\n",
    "\n",
    "        If `buckets` is a number, it will generate buckets which are\n",
    "        evenly spaced between the minimum and maximum of the RDD. For\n",
    "        example, if the min value is 0 and the max is 100, given `buckets`\n",
    "        as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n",
    "        be at least 1. An exception is raised if the RDD contains infinity.\n",
    "        If the elements in the RDD do not vary (max == min), a single bucket\n",
    "        will be used.\n",
    "\n",
    "        The return value is a tuple of buckets and histogram.\n",
    "\n",
    "        >>> rdd = sc.parallelize(range(51))\n",
    "        >>> rdd.histogram(2)\n",
    "        ([0, 25, 50], [25, 26])\n",
    "        >>> rdd.histogram([0, 5, 25, 50])\n",
    "        ([0, 5, 25, 50], [5, 20, 26])\n",
    "        >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n",
    "        ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
    "        >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n",
    "        >>> rdd.histogram((\"a\", \"b\", \"c\"))\n",
    "        (('a', 'b', 'c'), [2, 2])\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(buckets, int):\n",
    "            if buckets < 1:\n",
    "                raise ValueError(\"number of buckets must be >= 1\")\n",
    "\n",
    "            # filter out non-comparable elements\n",
    "            def comparable(x):\n",
    "                if x is None:\n",
    "                    return False\n",
    "                if type(x) is float and isnan(x):\n",
    "                    return False\n",
    "                return True\n",
    "\n",
    "            filtered = self.filter(comparable)\n",
    "\n",
    "            # faster than stats()\n",
    "            def minmax(a, b):\n",
    "                return min(a[0], b[0]), max(a[1], b[1])\n",
    "            try:\n",
    "                minv, maxv = filtered.map(lambda x: (x, x)).reduce(minmax)\n",
    "            except TypeError as e:\n",
    "                if \" empty \" in str(e):\n",
    "                    raise ValueError(\"can not generate buckets from empty RDD\")\n",
    "                raise\n",
    "\n",
    "            if minv == maxv or buckets == 1:\n",
    "                return [minv, maxv], [filtered.count()]\n",
    "\n",
    "            try:\n",
    "                inc = (maxv - minv) / buckets\n",
    "            except TypeError:\n",
    "                raise TypeError(\"Can not generate buckets with non-number in RDD\")\n",
    "\n",
    "            if isinf(inc):\n",
    "                raise ValueError(\"Can not generate buckets with infinite value\")\n",
    "\n",
    "            # keep them as integer if possible\n",
    "            inc = int(inc)\n",
    "            if inc * buckets != maxv - minv:\n",
    "                inc = (maxv - minv) * 1.0 / buckets\n",
    "\n",
    "            buckets = [i * inc + minv for i in range(buckets)]\n",
    "            buckets.append(maxv)  # fix accumulated error\n",
    "            even = True\n",
    "\n",
    "        elif isinstance(buckets, (list, tuple)):\n",
    "            if len(buckets) < 2:\n",
    "                raise ValueError(\"buckets should have more than one value\")\n",
    "\n",
    "            if any(i is None or isinstance(i, float) and isnan(i) for i in buckets):\n",
    "                raise ValueError(\"can not have None or NaN in buckets\")\n",
    "\n",
    "            if sorted(buckets) != list(buckets):\n",
    "                raise ValueError(\"buckets should be sorted\")\n",
    "\n",
    "            if len(set(buckets)) != len(buckets):\n",
    "                raise ValueError(\"buckets should not contain duplicated values\")\n",
    "\n",
    "            minv = buckets[0]\n",
    "            maxv = buckets[-1]\n",
    "            even = False\n",
    "            inc = None\n",
    "            try:\n",
    "                steps = [buckets[i + 1] - buckets[i] for i in range(len(buckets) - 1)]\n",
    "            except TypeError:\n",
    "                pass  # objects in buckets do not support '-'\n",
    "            else:\n",
    "                if max(steps) - min(steps) < 1e-10:  # handle precision errors\n",
    "                    even = True\n",
    "                    inc = (maxv - minv) / (len(buckets) - 1)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"buckets should be a list or tuple or number(int or long)\")\n",
    "\n",
    "        def histogram(iterator):\n",
    "            counters = [0] * len(buckets)\n",
    "            for i in iterator:\n",
    "                if i is None or (type(i) is float and isnan(i)) or i > maxv or i < minv:\n",
    "                    continue\n",
    "                t = (int((i - minv) / inc) if even\n",
    "                     else bisect.bisect_right(buckets, i) - 1)\n",
    "                counters[t] += 1\n",
    "            # add last two together\n",
    "            last = counters.pop()\n",
    "            counters[-1] += last\n",
    "            return [counters]\n",
    "\n",
    "        def mergeCounters(a, b):\n",
    "            return [i + j for i, j in zip(a, b)]\n",
    "\n",
    "        return buckets, self.mapPartitions(histogram).reduce(mergeCounters)\n",
    "\n",
    "    def mean(self):\n",
    "        \"\"\"\n",
    "        Compute the mean of this RDD's elements.\n",
    "\n",
    "        >>> sc.parallelize([1, 2, 3]).mean()\n",
    "        2.0\n",
    "        \"\"\"\n",
    "        return self.stats().mean()\n",
    "\n",
    "    def variance(self):\n",
    "        \"\"\"\n",
    "        Compute the variance of this RDD's elements.\n",
    "\n",
    "        >>> sc.parallelize([1, 2, 3]).variance()\n",
    "        0.666...\n",
    "        \"\"\"\n",
    "        return self.stats().variance()\n",
    "\n",
    "    def stdev(self):\n",
    "        \"\"\"\n",
    "        Compute the standard deviation of this RDD's elements.\n",
    "\n",
    "        >>> sc.parallelize([1, 2, 3]).stdev()\n",
    "        0.816...\n",
    "        \"\"\"\n",
    "        return self.stats().stdev()\n",
    "\n",
    "    def sampleStdev(self):\n",
    "        \"\"\"\n",
    "        Compute the sample standard deviation of this RDD's elements (which\n",
    "        corrects for bias in estimating the standard deviation by dividing by\n",
    "        N-1 instead of N).\n",
    "\n",
    "        >>> sc.parallelize([1, 2, 3]).sampleStdev()\n",
    "        1.0\n",
    "        \"\"\"\n",
    "        return self.stats().sampleStdev()\n",
    "\n",
    "    def sampleVariance(self):\n",
    "        \"\"\"\n",
    "        Compute the sample variance of this RDD's elements (which corrects\n",
    "        for bias in estimating the variance by dividing by N-1 instead of N).\n",
    "\n",
    "        >>> sc.parallelize([1, 2, 3]).sampleVariance()\n",
    "        1.0\n",
    "        \"\"\"\n",
    "        return self.stats().sampleVariance()\n",
    "\n",
    "    def countByValue(self):\n",
    "        \"\"\"\n",
    "        Return the count of each unique value in this RDD as a dictionary of\n",
    "        (value, count) pairs.\n",
    "\n",
    "        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n",
    "        [(1, 2), (2, 3)]\n",
    "        \"\"\"\n",
    "        def countPartition(iterator):\n",
    "            counts = defaultdict(int)\n",
    "            for obj in iterator:\n",
    "                counts[obj] += 1\n",
    "            yield counts\n",
    "\n",
    "        def mergeMaps(m1, m2):\n",
    "            for k, v in m2.items():\n",
    "                m1[k] += v\n",
    "            return m1\n",
    "        return self.mapPartitions(countPartition).reduce(mergeMaps)\n",
    "\n",
    "    def top(self, num, key=None):\n",
    "        \"\"\"\n",
    "        Get the top N elements from an RDD.\n",
    "\n",
    "        .. note:: This method should only be used if the resulting array is expected\n",
    "            to be small, as all the data is loaded into the driver's memory.\n",
    "\n",
    "        .. note:: It returns the list sorted in descending order.\n",
    "\n",
    "        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n",
    "        [12]\n",
    "        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n",
    "        [6, 5]\n",
    "        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n",
    "        [4, 3, 2]\n",
    "        \"\"\"\n",
    "        def topIterator(iterator):\n",
    "            yield heapq.nlargest(num, iterator, key=key)\n",
    "\n",
    "        def merge(a, b):\n",
    "            return heapq.nlargest(num, a + b, key=key)\n",
    "\n",
    "        return self.mapPartitions(topIterator).reduce(merge)\n",
    "\n",
    "    def takeOrdered(self, num, key=None):\n",
    "        \"\"\"\n",
    "        Get the N elements from an RDD ordered in ascending order or as\n",
    "        specified by the optional key function.\n",
    "\n",
    "        .. note:: this method should only be used if the resulting array is expected\n",
    "            to be small, as all the data is loaded into the driver's memory.\n",
    "\n",
    "        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
    "        [1, 2, 3, 4, 5, 6]\n",
    "        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
    "        [10, 9, 7, 6, 5, 4]\n",
    "        \"\"\"\n",
    "\n",
    "        def merge(a, b):\n",
    "            return heapq.nsmallest(num, a + b, key)\n",
    "\n",
    "        return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)\n",
    "\n",
    "    def take(self, num):\n",
    "        \"\"\"\n",
    "        Take the first num elements of the RDD.\n",
    "\n",
    "        It works by first scanning one partition, and use the results from\n",
    "        that partition to estimate the number of additional partitions needed\n",
    "        to satisfy the limit.\n",
    "\n",
    "        Translated from the Scala implementation in RDD#take().\n",
    "\n",
    "        .. note:: this method should only be used if the resulting array is expected\n",
    "            to be small, as all the data is loaded into the driver's memory.\n",
    "\n",
    "        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
    "        [2, 3]\n",
    "        >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
    "        [2, 3, 4, 5, 6]\n",
    "        >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
    "        [91, 92, 93]\n",
    "        \"\"\"\n",
    "        items = []\n",
    "        totalParts = self.getNumPartitions()\n",
    "        partsScanned = 0\n",
    "\n",
    "        while len(items) < num and partsScanned < totalParts:\n",
    "            # The number of partitions to try in this iteration.\n",
    "            # It is ok for this number to be greater than totalParts because\n",
    "            # we actually cap it at totalParts in runJob.\n",
    "            numPartsToTry = 1\n",
    "            if partsScanned > 0:\n",
    "                # If we didn't find any rows after the previous iteration,\n",
    "                # quadruple and retry.  Otherwise, interpolate the number of\n",
    "                # partitions we need to try, but overestimate it by 50%.\n",
    "                # We also cap the estimation in the end.\n",
    "                if len(items) == 0:\n",
    "                    numPartsToTry = partsScanned * 4\n",
    "                else:\n",
    "                    # the first paramter of max is >=1 whenever partsScanned >= 2\n",
    "                    numPartsToTry = int(1.5 * num * partsScanned / len(items)) - partsScanned\n",
    "                    numPartsToTry = min(max(numPartsToTry, 1), partsScanned * 4)\n",
    "\n",
    "            left = num - len(items)\n",
    "\n",
    "            def takeUpToNumLeft(iterator):\n",
    "                iterator = iter(iterator)\n",
    "                taken = 0\n",
    "                while taken < left:\n",
    "                    yield next(iterator)\n",
    "                    taken += 1\n",
    "\n",
    "            p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))\n",
    "            res = self.context.runJob(self, takeUpToNumLeft, p)\n",
    "\n",
    "            items += res\n",
    "            partsScanned += numPartsToTry\n",
    "\n",
    "        return items[:num]\n",
    "\n",
    "    def first(self):\n",
    "        \"\"\"\n",
    "        Return the first element in this RDD.\n",
    "\n",
    "        >>> sc.parallelize([2, 3, 4]).first()\n",
    "        2\n",
    "        >>> sc.parallelize([]).first()\n",
    "        Traceback (most recent call last):\n",
    "            ...\n",
    "        ValueError: RDD is empty\n",
    "        \"\"\"\n",
    "        rs = self.take(1)\n",
    "        if rs:\n",
    "            return rs[0]\n",
    "        raise ValueError(\"RDD is empty\")\n",
    "\n",
    "    def isEmpty(self):\n",
    "        \"\"\"\n",
    "        Returns true if and only if the RDD contains no elements at all.\n",
    "\n",
    "        .. note:: an RDD may be empty even when it has at least 1 partition.\n",
    "\n",
    "        >>> sc.parallelize([]).isEmpty()\n",
    "        True\n",
    "        >>> sc.parallelize([1]).isEmpty()\n",
    "        False\n",
    "        \"\"\"\n",
    "        return self.getNumPartitions() == 0 or len(self.take(1)) == 0\n",
    "\n",
    "    def saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None):\n",
    "        \"\"\"\n",
    "        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
    "        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n",
    "        converted for output using either user specified converters or, by default,\n",
    "        L{org.apache.spark.api.python.JavaToWritableConverter}.\n",
    "\n",
    "        :param conf: Hadoop job configuration, passed in as a dict\n",
    "        :param keyConverter: (None by default)\n",
    "        :param valueConverter: (None by default)\n",
    "        \"\"\"\n",
    "        jconf = self.ctx._dictToJavaMap(conf)\n",
    "        pickledRDD = self._pickled()\n",
    "        self.ctx._jvm.PythonRDD.saveAsHadoopDataset(pickledRDD._jrdd, True, jconf,\n",
    "                                                    keyConverter, valueConverter, True)\n",
    "\n",
    "    def saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None,\n",
    "                               keyConverter=None, valueConverter=None, conf=None):\n",
    "        \"\"\"\n",
    "        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
    "        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n",
    "        will be inferred if not specified. Keys and values are converted for output using either\n",
    "        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n",
    "        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n",
    "        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
    "\n",
    "        :param path: path to Hadoop file\n",
    "        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n",
    "               (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n",
    "        :param keyClass: fully qualified classname of key Writable class\n",
    "               (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
    "        :param valueClass: fully qualified classname of value Writable class\n",
    "               (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
    "        :param keyConverter: (None by default)\n",
    "        :param valueConverter: (None by default)\n",
    "        :param conf: Hadoop job configuration, passed in as a dict (None by default)\n",
    "        \"\"\"\n",
    "        jconf = self.ctx._dictToJavaMap(conf)\n",
    "        pickledRDD = self._pickled()\n",
    "        self.ctx._jvm.PythonRDD.saveAsNewAPIHadoopFile(pickledRDD._jrdd, True, path,\n",
    "                                                       outputFormatClass,\n",
    "                                                       keyClass, valueClass,\n",
    "                                                       keyConverter, valueConverter, jconf)\n",
    "\n",
    "    def saveAsHadoopDataset(self, conf, keyConverter=None, valueConverter=None):\n",
    "        \"\"\"\n",
    "        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
    "        system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n",
    "        converted for output using either user specified converters or, by default,\n",
    "        L{org.apache.spark.api.python.JavaToWritableConverter}.\n",
    "\n",
    "        :param conf: Hadoop job configuration, passed in as a dict\n",
    "        :param keyConverter: (None by default)\n",
    "        :param valueConverter: (None by default)\n",
    "        \"\"\"\n",
    "        jconf = self.ctx._dictToJavaMap(conf)\n",
    "        pickledRDD = self._pickled()\n",
    "        self.ctx._jvm.PythonRDD.saveAsHadoopDataset(pickledRDD._jrdd, True, jconf,\n",
    "                                                    keyConverter, valueConverter, False)\n",
    "\n",
    "    def saveAsHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None,\n",
    "                         keyConverter=None, valueConverter=None, conf=None,\n",
    "                         compressionCodecClass=None):\n",
    "        \"\"\"\n",
    "        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
    "        system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n",
    "        will be inferred if not specified. Keys and values are converted for output using either\n",
    "        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n",
    "        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n",
    "        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
    "\n",
    "        :param path: path to Hadoop file\n",
    "        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n",
    "               (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n",
    "        :param keyClass: fully qualified classname of key Writable class\n",
    "               (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
    "        :param valueClass: fully qualified classname of value Writable class\n",
    "               (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
    "        :param keyConverter: (None by default)\n",
    "        :param valueConverter: (None by default)\n",
    "        :param conf: (None by default)\n",
    "        :param compressionCodecClass: (None by default)\n",
    "        \"\"\"\n",
    "        jconf = self.ctx._dictToJavaMap(conf)\n",
    "        pickledRDD = self._pickled()\n",
    "        self.ctx._jvm.PythonRDD.saveAsHadoopFile(pickledRDD._jrdd, True, path,\n",
    "                                                 outputFormatClass,\n",
    "                                                 keyClass, valueClass,\n",
    "                                                 keyConverter, valueConverter,\n",
    "                                                 jconf, compressionCodecClass)\n",
    "\n",
    "    def saveAsSequenceFile(self, path, compressionCodecClass=None):\n",
    "        \"\"\"\n",
    "        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
    "        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the\n",
    "        RDD's key and value types. The mechanism is as follows:\n",
    "\n",
    "            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n",
    "            2. Keys and values of this Java RDD are converted to Writables and written out.\n",
    "\n",
    "        :param path: path to sequence file\n",
    "        :param compressionCodecClass: (None by default)\n",
    "        \"\"\"\n",
    "        pickledRDD = self._pickled()\n",
    "        self.ctx._jvm.PythonRDD.saveAsSequenceFile(pickledRDD._jrdd, True,\n",
    "                                                   path, compressionCodecClass)\n",
    "\n",
    "    def saveAsPickleFile(self, path, batchSize=10):\n",
    "        \"\"\"\n",
    "        Save this RDD as a SequenceFile of serialized objects. The serializer\n",
    "        used is L{pyspark.serializers.PickleSerializer}, default batch size\n",
    "        is 10.\n",
    "\n",
    "        >>> tmpFile = NamedTemporaryFile(delete=True)\n",
    "        >>> tmpFile.close()\n",
    "        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n",
    "        >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n",
    "        ['1', '2', 'rdd', 'spark']\n",
    "        \"\"\"\n",
    "        if batchSize == 0:\n",
    "            ser = AutoBatchedSerializer(PickleSerializer())\n",
    "        else:\n",
    "            ser = BatchedSerializer(PickleSerializer(), batchSize)\n",
    "        self._reserialize(ser)._jrdd.saveAsObjectFile(path)\n",
    "\n",
    "    @ignore_unicode_prefix\n",
    "    def saveAsTextFile(self, path, compressionCodecClass=None):\n",
    "        \"\"\"\n",
    "        Save this RDD as a text file, using string representations of elements.\n",
    "\n",
    "        @param path: path to text file\n",
    "        @param compressionCodecClass: (None by default) string i.e.\n",
    "            \"org.apache.hadoop.io.compress.GzipCodec\"\n",
    "\n",
    "        >>> tempFile = NamedTemporaryFile(delete=True)\n",
    "        >>> tempFile.close()\n",
    "        >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n",
    "        >>> from fileinput import input\n",
    "        >>> from glob import glob\n",
    "        >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n",
    "        '0\\\\n1\\\\n2\\\\n3\\\\n4\\\\n5\\\\n6\\\\n7\\\\n8\\\\n9\\\\n'\n",
    "\n",
    "        Empty lines are tolerated when saving to text files.\n",
    "\n",
    "        >>> tempFile2 = NamedTemporaryFile(delete=True)\n",
    "        >>> tempFile2.close()\n",
    "        >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n",
    "        >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n",
    "        '\\\\n\\\\n\\\\nbar\\\\nfoo\\\\n'\n",
    "\n",
    "        Using compressionCodecClass\n",
    "\n",
    "        >>> tempFile3 = NamedTemporaryFile(delete=True)\n",
    "        >>> tempFile3.close()\n",
    "        >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
    "        >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n",
    "        >>> from fileinput import input, hook_compressed\n",
    "        >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n",
    "        >>> b''.join(result).decode('utf-8')\n",
    "        u'bar\\\\nfoo\\\\n'\n",
    "        \"\"\"\n",
    "        def func(split, iterator):\n",
    "            for x in iterator:\n",
    "                if not isinstance(x, (unicode, bytes)):\n",
    "                    x = unicode(x)\n",
    "                if isinstance(x, unicode):\n",
    "                    x = x.encode(\"utf-8\")\n",
    "                yield x\n",
    "        keyed = self.mapPartitionsWithIndex(func)\n",
    "        keyed._bypass_serializer = True\n",
    "        if compressionCodecClass:\n",
    "            compressionCodec = self.ctx._jvm.java.lang.Class.forName(compressionCodecClass)\n",
    "            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path, compressionCodec)\n",
    "        else:\n",
    "            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n",
    "\n",
    "    # Pair functions\n",
    "\n",
    "    def collectAsMap(self):\n",
    "        \"\"\"\n",
    "        Return the key-value pairs in this RDD to the master as a dictionary.\n",
    "\n",
    "        .. note:: this method should only be used if the resulting data is expected\n",
    "            to be small, as all the data is loaded into the driver's memory.\n",
    "\n",
    "        >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
    "        >>> m[1]\n",
    "        2\n",
    "        >>> m[3]\n",
    "        4\n",
    "        \"\"\"\n",
    "        return dict(self.collect())\n",
    "\n",
    "    def keys(self):\n",
    "        \"\"\"\n",
    "        Return an RDD with the keys of each tuple.\n",
    "\n",
    "        >>> m = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
    "        >>> m.collect()\n",
    "        [1, 3]\n",
    "        \"\"\"\n",
    "        return self.map(lambda x: x[0])\n",
    "\n",
    "    def values(self):\n",
    "        \"\"\"\n",
    "        Return an RDD with the values of each tuple.\n",
    "\n",
    "        >>> m = sc.parallelize([(1, 2), (3, 4)]).values()\n",
    "        >>> m.collect()\n",
    "        [2, 4]\n",
    "        \"\"\"\n",
    "        return self.map(lambda x: x[1])\n",
    "\n",
    "    def reduceByKey(self, func, numPartitions=None, partitionFunc=portable_hash):\n",
    "        \"\"\"\n",
    "        Merge the values for each key using an associative and commutative reduce function.\n",
    "\n",
    "        This will also perform the merging locally on each mapper before\n",
    "        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
    "\n",
    "        Output will be partitioned with C{numPartitions} partitions, or\n",
    "        the default parallelism level if C{numPartitions} is not specified.\n",
    "        Default partitioner is hash-partition.\n",
    "\n",
    "        >>> from operator import add\n",
    "        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "        >>> sorted(rdd.reduceByKey(add).collect())\n",
    "        [('a', 2), ('b', 1)]\n",
    "        \"\"\"\n",
    "        return self.combineByKey(lambda x: x, func, func, numPartitions, partitionFunc)\n",
    "\n",
    "    def reduceByKeyLocally(self, func):\n",
    "        \"\"\"\n",
    "        Merge the values for each key using an associative and commutative reduce function, but\n",
    "        return the results immediately to the master as a dictionary.\n",
    "\n",
    "        This will also perform the merging locally on each mapper before\n",
    "        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
    "\n",
    "        >>> from operator import add\n",
    "        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "        >>> sorted(rdd.reduceByKeyLocally(add).items())\n",
    "        [('a', 2), ('b', 1)]\n",
    "        \"\"\"\n",
    "        def reducePartition(iterator):\n",
    "            m = {}\n",
    "            for k, v in iterator:\n",
    "                m[k] = func(m[k], v) if k in m else v\n",
    "            yield m\n",
    "\n",
    "        def mergeMaps(m1, m2):\n",
    "            for k, v in m2.items():\n",
    "                m1[k] = func(m1[k], v) if k in m1 else v\n",
    "            return m1\n",
    "        return self.mapPartitions(reducePartition).reduce(mergeMaps)\n",
    "\n",
    "    def countByKey(self):\n",
    "        \"\"\"\n",
    "        Count the number of elements for each key, and return the result to the\n",
    "        master as a dictionary.\n",
    "\n",
    "        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "        >>> sorted(rdd.countByKey().items())\n",
    "        [('a', 2), ('b', 1)]\n",
    "        \"\"\"\n",
    "        return self.map(lambda x: x[0]).countByValue()\n",
    "\n",
    "    def join(self, other, numPartitions=None):\n",
    "        \"\"\"\n",
    "        Return an RDD containing all pairs of elements with matching keys in\n",
    "        C{self} and C{other}.\n",
    "\n",
    "        Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
    "        (k, v1) is in C{self} and (k, v2) is in C{other}.\n",
    "\n",
    "        Performs a hash join across the cluster.\n",
    "\n",
    "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "        >>> y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "        >>> sorted(x.join(y).collect())\n",
    "        [('a', (1, 2)), ('a', (1, 3))]\n",
    "        \"\"\"\n",
    "        return python_join(self, other, numPartitions)\n",
    "\n",
    "    def leftOuterJoin(self, other, numPartitions=None):\n",
    "        \"\"\"\n",
    "        Perform a left outer join of C{self} and C{other}.\n",
    "\n",
    "        For each element (k, v) in C{self}, the resulting RDD will either\n",
    "        contain all pairs (k, (v, w)) for w in C{other}, or the pair\n",
    "        (k, (v, None)) if no elements in C{other} have key k.\n",
    "\n",
    "        Hash-partitions the resulting RDD into the given number of partitions.\n",
    "\n",
    "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "        >>> y = sc.parallelize([(\"a\", 2)])\n",
    "        >>> sorted(x.leftOuterJoin(y).collect())\n",
    "        [('a', (1, 2)), ('b', (4, None))]\n",
    "        \"\"\"\n",
    "        return python_left_outer_join(self, other, numPartitions)\n",
    "\n",
    "    def rightOuterJoin(self, other, numPartitions=None):\n",
    "        \"\"\"\n",
    "        Perform a right outer join of C{self} and C{other}.\n",
    "\n",
    "        For each element (k, w) in C{other}, the resulting RDD will either\n",
    "        contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n",
    "        if no elements in C{self} have key k.\n",
    "\n",
    "        Hash-partitions the resulting RDD into the given number of partitions.\n",
    "\n",
    "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "        >>> y = sc.parallelize([(\"a\", 2)])\n",
    "        >>> sorted(y.rightOuterJoin(x).collect())\n",
    "        [('a', (2, 1)), ('b', (None, 4))]\n",
    "        \"\"\"\n",
    "        return python_right_outer_join(self, other, numPartitions)\n",
    "\n",
    "    def fullOuterJoin(self, other, numPartitions=None):\n",
    "        \"\"\"\n",
    "        Perform a right outer join of C{self} and C{other}.\n",
    "\n",
    "        For each element (k, v) in C{self}, the resulting RDD will either\n",
    "        contain all pairs (k, (v, w)) for w in C{other}, or the pair\n",
    "        (k, (v, None)) if no elements in C{other} have key k.\n",
    "\n",
    "        Similarly, for each element (k, w) in C{other}, the resulting RDD will\n",
    "        either contain all pairs (k, (v, w)) for v in C{self}, or the pair\n",
    "        (k, (None, w)) if no elements in C{self} have key k.\n",
    "\n",
    "        Hash-partitions the resulting RDD into the given number of partitions.\n",
    "\n",
    "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "        >>> y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
    "        >>> sorted(x.fullOuterJoin(y).collect())\n",
    "        [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n",
    "        \"\"\"\n",
    "        return python_full_outer_join(self, other, numPartitions)\n",
    "\n",
    "    # TODO: add option to control map-side combining\n",
    "    # portable_hash is used as default, because builtin hash of None is different\n",
    "    # cross machines.\n",
    "    def partitionBy(self, numPartitions, partitionFunc=portable_hash):\n",
    "        \"\"\"\n",
    "        Return a copy of the RDD partitioned using the specified partitioner.\n",
    "\n",
    "        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
    "        >>> sets = pairs.partitionBy(2).glom().collect()\n",
    "        >>> len(set(sets[0]).intersection(set(sets[1])))\n",
    "        0\n",
    "        \"\"\"\n",
    "        if numPartitions is None:\n",
    "            numPartitions = self._defaultReducePartitions()\n",
    "        partitioner = Partitioner(numPartitions, partitionFunc)\n",
    "        if self.partitioner == partitioner:\n",
    "            return self\n",
    "\n",
    "        # Transferring O(n) objects to Java is too expensive.\n",
    "        # Instead, we'll form the hash buckets in Python,\n",
    "        # transferring O(numPartitions) objects to Java.\n",
    "        # Each object is a (splitNumber, [objects]) pair.\n",
    "        # In order to avoid too huge objects, the objects are\n",
    "        # grouped into chunks.\n",
    "        outputSerializer = self.ctx._unbatched_serializer\n",
    "\n",
    "        limit = (_parse_memory(self.ctx._conf.get(\n",
    "            \"spark.python.worker.memory\", \"512m\")) / 2)\n",
    "\n",
    "        def add_shuffle_key(split, iterator):\n",
    "\n",
    "            buckets = defaultdict(list)\n",
    "            c, batch = 0, min(10 * numPartitions, 1000)\n",
    "\n",
    "            for k, v in iterator:\n",
    "                buckets[partitionFunc(k) % numPartitions].append((k, v))\n",
    "                c += 1\n",
    "\n",
    "                # check used memory and avg size of chunk of objects\n",
    "                if (c % 1000 == 0 and get_used_memory() > limit\n",
    "                        or c > batch):\n",
    "                    n, size = len(buckets), 0\n",
    "                    for split in list(buckets.keys()):\n",
    "                        yield pack_long(split)\n",
    "                        d = outputSerializer.dumps(buckets[split])\n",
    "                        del buckets[split]\n",
    "                        yield d\n",
    "                        size += len(d)\n",
    "\n",
    "                    avg = int(size / n) >> 20\n",
    "                    # let 1M < avg < 10M\n",
    "                    if avg < 1:\n",
    "                        batch *= 1.5\n",
    "                    elif avg > 10:\n",
    "                        batch = max(int(batch / 1.5), 1)\n",
    "                    c = 0\n",
    "\n",
    "            for split, items in buckets.items():\n",
    "                yield pack_long(split)\n",
    "                yield outputSerializer.dumps(items)\n",
    "\n",
    "        keyed = self.mapPartitionsWithIndex(add_shuffle_key, preservesPartitioning=True)\n",
    "        keyed._bypass_serializer = True\n",
    "        with SCCallSiteSync(self.context) as css:\n",
    "            pairRDD = self.ctx._jvm.PairwiseRDD(\n",
    "                keyed._jrdd.rdd()).asJavaPairRDD()\n",
    "            jpartitioner = self.ctx._jvm.PythonPartitioner(numPartitions,\n",
    "                                                           id(partitionFunc))\n",
    "        jrdd = self.ctx._jvm.PythonRDD.valueOfPair(pairRDD.partitionBy(jpartitioner))\n",
    "        rdd = RDD(jrdd, self.ctx, BatchedSerializer(outputSerializer))\n",
    "        rdd.partitioner = partitioner\n",
    "        return rdd\n",
    "\n",
    "    # TODO: add control over map-side aggregation\n",
    "    def combineByKey(self, createCombiner, mergeValue, mergeCombiners,\n",
    "                     numPartitions=None, partitionFunc=portable_hash):\n",
    "        \"\"\"\n",
    "        Generic function to combine the elements for each key using a custom\n",
    "        set of aggregation functions.\n",
    "\n",
    "        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n",
    "        type\" C.\n",
    "\n",
    "        Users provide three functions:\n",
    "\n",
    "            - C{createCombiner}, which turns a V into a C (e.g., creates\n",
    "              a one-element list)\n",
    "            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of\n",
    "              a list)\n",
    "            - C{mergeCombiners}, to combine two C's into a single one (e.g., merges\n",
    "              the lists)\n",
    "\n",
    "        To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n",
    "        modify and return their first argument instead of creating a new C.\n",
    "\n",
    "        In addition, users can control the partitioning of the output RDD.\n",
    "\n",
    "        .. note:: V and C can be different -- for example, one might group an RDD of type\n",
    "            (Int, Int) into an RDD of type (Int, List[Int]).\n",
    "\n",
    "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
    "        >>> def to_list(a):\n",
    "        ...     return [a]\n",
    "        ...\n",
    "        >>> def append(a, b):\n",
    "        ...     a.append(b)\n",
    "        ...     return a\n",
    "        ...\n",
    "        >>> def extend(a, b):\n",
    "        ...     a.extend(b)\n",
    "        ...     return a\n",
    "        ...\n",
    "        >>> sorted(x.combineByKey(to_list, append, extend).collect())\n",
    "        [('a', [1, 2]), ('b', [1])]\n",
    "        \"\"\"\n",
    "        if numPartitions is None:\n",
    "            numPartitions = self._defaultReducePartitions()\n",
    "\n",
    "        serializer = self.ctx.serializer\n",
    "        memory = self._memory_limit()\n",
    "        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)\n",
    "\n",
    "        def combineLocally(iterator):\n",
    "            merger = ExternalMerger(agg, memory * 0.9, serializer)\n",
    "            merger.mergeValues(iterator)\n",
    "            return merger.items()\n",
    "\n",
    "        locally_combined = self.mapPartitions(combineLocally, preservesPartitioning=True)\n",
    "        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)\n",
    "\n",
    "        def _mergeCombiners(iterator):\n",
    "            merger = ExternalMerger(agg, memory, serializer)\n",
    "            merger.mergeCombiners(iterator)\n",
    "            return merger.items()\n",
    "\n",
    "        return shuffled.mapPartitions(_mergeCombiners, preservesPartitioning=True)\n",
    "\n",
    "    def aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None,\n",
    "                       partitionFunc=portable_hash):\n",
    "        \"\"\"\n",
    "        Aggregate the values of each key, using given combine functions and a neutral\n",
    "        \"zero value\". This function can return a different result type, U, than the type\n",
    "        of the values in this RDD, V. Thus, we need one operation for merging a V into\n",
    "        a U and one operation for merging two U's, The former operation is used for merging\n",
    "        values within a partition, and the latter is used for merging values between\n",
    "        partitions. To avoid memory allocation, both of these functions are\n",
    "        allowed to modify and return their first argument instead of creating a new U.\n",
    "        \"\"\"\n",
    "        def createZero():\n",
    "            return copy.deepcopy(zeroValue)\n",
    "\n",
    "        return self.combineByKey(\n",
    "            lambda v: seqFunc(createZero(), v), seqFunc, combFunc, numPartitions, partitionFunc)\n",
    "\n",
    "    def foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=portable_hash):\n",
    "        \"\"\"\n",
    "        Merge the values for each key using an associative function \"func\"\n",
    "        and a neutral \"zeroValue\" which may be added to the result an\n",
    "        arbitrary number of times, and must not change the result\n",
    "        (e.g., 0 for addition, or 1 for multiplication.).\n",
    "\n",
    "        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "        >>> from operator import add\n",
    "        >>> sorted(rdd.foldByKey(0, add).collect())\n",
    "        [('a', 2), ('b', 1)]\n",
    "        \"\"\"\n",
    "        def createZero():\n",
    "            return copy.deepcopy(zeroValue)\n",
    "\n",
    "        return self.combineByKey(lambda v: func(createZero(), v), func, func, numPartitions,\n",
    "                                 partitionFunc)\n",
    "\n",
    "    def _memory_limit(self):\n",
    "        return _parse_memory(self.ctx._conf.get(\"spark.python.worker.memory\", \"512m\"))\n",
    "\n",
    "    # TODO: support variant with custom partitioner\n",
    "    def groupByKey(self, numPartitions=None, partitionFunc=portable_hash):\n",
    "        \"\"\"\n",
    "        Group the values for each key in the RDD into a single sequence.\n",
    "        Hash-partitions the resulting RDD with numPartitions partitions.\n",
    "\n",
    "        .. note:: If you are grouping in order to perform an aggregation (such as a\n",
    "            sum or average) over each key, using reduceByKey or aggregateByKey will\n",
    "            provide much better performance.\n",
    "\n",
    "        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "        >>> sorted(rdd.groupByKey().mapValues(len).collect())\n",
    "        [('a', 2), ('b', 1)]\n",
    "        >>> sorted(rdd.groupByKey().mapValues(list).collect())\n",
    "        [('a', [1, 1]), ('b', [1])]\n",
    "        \"\"\"\n",
    "        def createCombiner(x):\n",
    "            return [x]\n",
    "\n",
    "        def mergeValue(xs, x):\n",
    "            xs.append(x)\n",
    "            return xs\n",
    "\n",
    "        def mergeCombiners(a, b):\n",
    "            a.extend(b)\n",
    "            return a\n",
    "\n",
    "        memory = self._memory_limit()\n",
    "        serializer = self._jrdd_deserializer\n",
    "        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)\n",
    "\n",
    "        def combine(iterator):\n",
    "            merger = ExternalMerger(agg, memory * 0.9, serializer)\n",
    "            merger.mergeValues(iterator)\n",
    "            return merger.items()\n",
    "\n",
    "        locally_combined = self.mapPartitions(combine, preservesPartitioning=True)\n",
    "        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)\n",
    "\n",
    "        def groupByKey(it):\n",
    "            merger = ExternalGroupBy(agg, memory, serializer)\n",
    "            merger.mergeCombiners(it)\n",
    "            return merger.items()\n",
    "\n",
    "        return shuffled.mapPartitions(groupByKey, True).mapValues(ResultIterable)\n",
    "\n",
    "    def flatMapValues(self, f):\n",
    "        \"\"\"\n",
    "        Pass each value in the key-value pair RDD through a flatMap function\n",
    "        without changing the keys; this also retains the original RDD's\n",
    "        partitioning.\n",
    "\n",
    "        >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
    "        >>> def f(x): return x\n",
    "        >>> x.flatMapValues(f).collect()\n",
    "        [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
    "        \"\"\"\n",
    "        flat_map_fn = lambda kv: ((kv[0], x) for x in f(kv[1]))\n",
    "        return self.flatMap(flat_map_fn, preservesPartitioning=True)\n",
    "\n",
    "    def mapValues(self, f):\n",
    "        \"\"\"\n",
    "        Pass each value in the key-value pair RDD through a map function\n",
    "        without changing the keys; this also retains the original RDD's\n",
    "        partitioning.\n",
    "\n",
    "        >>> x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
    "        >>> def f(x): return len(x)\n",
    "        >>> x.mapValues(f).collect()\n",
    "        [('a', 3), ('b', 1)]\n",
    "        \"\"\"\n",
    "        map_values_fn = lambda kv: (kv[0], f(kv[1]))\n",
    "        return self.map(map_values_fn, preservesPartitioning=True)\n",
    "\n",
    "    def groupWith(self, other, *others):\n",
    "        \"\"\"\n",
    "        Alias for cogroup but with support for multiple RDDs.\n",
    "\n",
    "        >>> w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
    "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "        >>> y = sc.parallelize([(\"a\", 2)])\n",
    "        >>> z = sc.parallelize([(\"b\", 42)])\n",
    "        >>> [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]\n",
    "        [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n",
    "\n",
    "        \"\"\"\n",
    "        return python_cogroup((self, other) + others, numPartitions=None)\n",
    "\n",
    "    # TODO: add variant with custom parittioner\n",
    "    def cogroup(self, other, numPartitions=None):\n",
    "        \"\"\"\n",
    "        For each key k in C{self} or C{other}, return a resulting RDD that\n",
    "        contains a tuple with the list of values for that key in C{self} as\n",
    "        well as C{other}.\n",
    "\n",
    "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "        >>> y = sc.parallelize([(\"a\", 2)])\n",
    "        >>> [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n",
    "        [('a', ([1], [2])), ('b', ([4], []))]\n",
    "        \"\"\"\n",
    "        return python_cogroup((self, other), numPartitions)\n",
    "\n",
    "    def sampleByKey(self, withReplacement, fractions, seed=None):\n",
    "        \"\"\"\n",
    "        Return a subset of this RDD sampled by key (via stratified sampling).\n",
    "        Create a sample of this RDD using variable sampling rates for\n",
    "        different keys as specified by fractions, a key to sampling rate map.\n",
    "\n",
    "        >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n",
    "        >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
    "        >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
    "        >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n",
    "        True\n",
    "        >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n",
    "        True\n",
    "        >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n",
    "        True\n",
    "        \"\"\"\n",
    "        for fraction in fractions.values():\n",
    "            assert fraction >= 0.0, \"Negative fraction value: %s\" % fraction\n",
    "        return self.mapPartitionsWithIndex(\n",
    "            RDDStratifiedSampler(withReplacement, fractions, seed).func, True)\n",
    "\n",
    "    def subtractByKey(self, other, numPartitions=None):\n",
    "        \"\"\"\n",
    "        Return each (key, value) pair in C{self} that has no pair with matching\n",
    "        key in C{other}.\n",
    "\n",
    "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
    "        >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "        >>> sorted(x.subtractByKey(y).collect())\n",
    "        [('b', 4), ('b', 5)]\n",
    "        \"\"\"\n",
    "        def filter_func(pair):\n",
    "            key, (val1, val2) = pair\n",
    "            return val1 and not val2\n",
    "        return self.cogroup(other, numPartitions).filter(filter_func).flatMapValues(lambda x: x[0])\n",
    "\n",
    "    def subtract(self, other, numPartitions=None):\n",
    "        \"\"\"\n",
    "        Return each value in C{self} that is not contained in C{other}.\n",
    "\n",
    "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
    "        >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "        >>> sorted(x.subtract(y).collect())\n",
    "        [('a', 1), ('b', 4), ('b', 5)]\n",
    "        \"\"\"\n",
    "        # note: here 'True' is just a placeholder\n",
    "        rdd = other.map(lambda x: (x, True))\n",
    "        return self.map(lambda x: (x, True)).subtractByKey(rdd, numPartitions).keys()\n",
    "\n",
    "    def keyBy(self, f):\n",
    "        \"\"\"\n",
    "        Creates tuples of the elements in this RDD by applying C{f}.\n",
    "\n",
    "        >>> x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
    "        >>> y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
    "        >>> [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]\n",
    "        [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n",
    "        \"\"\"\n",
    "        return self.map(lambda x: (f(x), x))\n",
    "\n",
    "    def repartition(self, numPartitions):\n",
    "        \"\"\"\n",
    "         Return a new RDD that has exactly numPartitions partitions.\n",
    "\n",
    "         Can increase or decrease the level of parallelism in this RDD.\n",
    "         Internally, this uses a shuffle to redistribute data.\n",
    "         If you are decreasing the number of partitions in this RDD, consider\n",
    "         using `coalesce`, which can avoid performing a shuffle.\n",
    "\n",
    "         >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
    "         >>> sorted(rdd.glom().collect())\n",
    "         [[1], [2, 3], [4, 5], [6, 7]]\n",
    "         >>> len(rdd.repartition(2).glom().collect())\n",
    "         2\n",
    "         >>> len(rdd.repartition(10).glom().collect())\n",
    "         10\n",
    "        \"\"\"\n",
    "        return self.coalesce(numPartitions, shuffle=True)\n",
    "\n",
    "    def coalesce(self, numPartitions, shuffle=False):\n",
    "        \"\"\"\n",
    "        Return a new RDD that is reduced into `numPartitions` partitions.\n",
    "\n",
    "        >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n",
    "        [[1], [2, 3], [4, 5]]\n",
    "        >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n",
    "        [[1, 2, 3, 4, 5]]\n",
    "        \"\"\"\n",
    "        if shuffle:\n",
    "            # Decrease the batch size in order to distribute evenly the elements across output\n",
    "            # partitions. Otherwise, repartition will possibly produce highly skewed partitions.\n",
    "            batchSize = min(10, self.ctx._batchSize or 1024)\n",
    "            ser = BatchedSerializer(PickleSerializer(), batchSize)\n",
    "            selfCopy = self._reserialize(ser)\n",
    "            jrdd_deserializer = selfCopy._jrdd_deserializer\n",
    "            jrdd = selfCopy._jrdd.coalesce(numPartitions, shuffle)\n",
    "        else:\n",
    "            jrdd_deserializer = self._jrdd_deserializer\n",
    "            jrdd = self._jrdd.coalesce(numPartitions, shuffle)\n",
    "        return RDD(jrdd, self.ctx, jrdd_deserializer)\n",
    "\n",
    "    def zip(self, other):\n",
    "        \"\"\"\n",
    "        Zips this RDD with another one, returning key-value pairs with the\n",
    "        first element in each RDD second element in each RDD, etc. Assumes\n",
    "        that the two RDDs have the same number of partitions and the same\n",
    "        number of elements in each partition (e.g. one was made through\n",
    "        a map on the other).\n",
    "\n",
    "        >>> x = sc.parallelize(range(0,5))\n",
    "        >>> y = sc.parallelize(range(1000, 1005))\n",
    "        >>> x.zip(y).collect()\n",
    "        [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n",
    "        \"\"\"\n",
    "        def get_batch_size(ser):\n",
    "            if isinstance(ser, BatchedSerializer):\n",
    "                return ser.batchSize\n",
    "            return 1  # not batched\n",
    "\n",
    "        def batch_as(rdd, batchSize):\n",
    "            return rdd._reserialize(BatchedSerializer(PickleSerializer(), batchSize))\n",
    "\n",
    "        my_batch = get_batch_size(self._jrdd_deserializer)\n",
    "        other_batch = get_batch_size(other._jrdd_deserializer)\n",
    "        if my_batch != other_batch or not my_batch:\n",
    "            # use the smallest batchSize for both of them\n",
    "            batchSize = min(my_batch, other_batch)\n",
    "            if batchSize <= 0:\n",
    "                # auto batched or unlimited\n",
    "                batchSize = 100\n",
    "            other = batch_as(other, batchSize)\n",
    "            self = batch_as(self, batchSize)\n",
    "\n",
    "        if self.getNumPartitions() != other.getNumPartitions():\n",
    "            raise ValueError(\"Can only zip with RDD which has the same number of partitions\")\n",
    "\n",
    "        # There will be an Exception in JVM if there are different number\n",
    "        # of items in each partitions.\n",
    "        pairRDD = self._jrdd.zip(other._jrdd)\n",
    "        deserializer = PairDeserializer(self._jrdd_deserializer,\n",
    "                                        other._jrdd_deserializer)\n",
    "        return RDD(pairRDD, self.ctx, deserializer)\n",
    "\n",
    "    def zipWithIndex(self):\n",
    "        \"\"\"\n",
    "        Zips this RDD with its element indices.\n",
    "\n",
    "        The ordering is first based on the partition index and then the\n",
    "        ordering of items within each partition. So the first item in\n",
    "        the first partition gets index 0, and the last item in the last\n",
    "        partition receives the largest index.\n",
    "\n",
    "        This method needs to trigger a spark job when this RDD contains\n",
    "        more than one partitions.\n",
    "\n",
    "        >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n",
    "        [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n",
    "        \"\"\"\n",
    "        starts = [0]\n",
    "        if self.getNumPartitions() > 1:\n",
    "            nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()\n",
    "            for i in range(len(nums) - 1):\n",
    "                starts.append(starts[-1] + nums[i])\n",
    "\n",
    "        def func(k, it):\n",
    "            for i, v in enumerate(it, starts[k]):\n",
    "                yield v, i\n",
    "\n",
    "        return self.mapPartitionsWithIndex(func)\n",
    "\n",
    "    def zipWithUniqueId(self):\n",
    "        \"\"\"\n",
    "        Zips this RDD with generated unique Long ids.\n",
    "\n",
    "        Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n",
    "        n is the number of partitions. So there may exist gaps, but this\n",
    "        method won't trigger a spark job, which is different from\n",
    "        L{zipWithIndex}\n",
    "\n",
    "        >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n",
    "        [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n",
    "        \"\"\"\n",
    "        n = self.getNumPartitions()\n",
    "\n",
    "        def func(k, it):\n",
    "            for i, v in enumerate(it):\n",
    "                yield v, i * n + k\n",
    "\n",
    "        return self.mapPartitionsWithIndex(func)\n",
    "\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Return the name of this RDD.\n",
    "        \"\"\"\n",
    "        n = self._jrdd.name()\n",
    "        if n:\n",
    "            return n\n",
    "\n",
    "    @ignore_unicode_prefix\n",
    "    def setName(self, name):\n",
    "        \"\"\"\n",
    "        Assign a name to this RDD.\n",
    "\n",
    "        >>> rdd1 = sc.parallelize([1, 2])\n",
    "        >>> rdd1.setName('RDD1').name()\n",
    "        u'RDD1'\n",
    "        \"\"\"\n",
    "        self._jrdd.setName(name)\n",
    "        return self\n",
    "\n",
    "    def toDebugString(self):\n",
    "        \"\"\"\n",
    "        A description of this RDD and its recursive dependencies for debugging.\n",
    "        \"\"\"\n",
    "        debug_string = self._jrdd.toDebugString()\n",
    "        if debug_string:\n",
    "            return debug_string.encode('utf-8')\n",
    "\n",
    "    def getStorageLevel(self):\n",
    "        \"\"\"\n",
    "        Get the RDD's current storage level.\n",
    "\n",
    "        >>> rdd1 = sc.parallelize([1,2])\n",
    "        >>> rdd1.getStorageLevel()\n",
    "        StorageLevel(False, False, False, False, 1)\n",
    "        >>> print(rdd1.getStorageLevel())\n",
    "        Serialized 1x Replicated\n",
    "        \"\"\"\n",
    "        java_storage_level = self._jrdd.getStorageLevel()\n",
    "        storage_level = StorageLevel(java_storage_level.useDisk(),\n",
    "                                     java_storage_level.useMemory(),\n",
    "                                     java_storage_level.useOffHeap(),\n",
    "                                     java_storage_level.deserialized(),\n",
    "                                     java_storage_level.replication())\n",
    "        return storage_level\n",
    "\n",
    "    def _defaultReducePartitions(self):\n",
    "        \"\"\"\n",
    "        Returns the default number of partitions to use during reduce tasks (e.g., groupBy).\n",
    "        If spark.default.parallelism is set, then we'll use the value from SparkContext\n",
    "        defaultParallelism, otherwise we'll use the number of partitions in this RDD.\n",
    "\n",
    "        This mirrors the behavior of the Scala Partitioner#defaultPartitioner, intended to reduce\n",
    "        the likelihood of OOMs. Once PySpark adopts Partitioner-based APIs, this behavior will\n",
    "        be inherent.\n",
    "        \"\"\"\n",
    "        if self.ctx._conf.contains(\"spark.default.parallelism\"):\n",
    "            return self.ctx.defaultParallelism\n",
    "        else:\n",
    "            return self.getNumPartitions()\n",
    "\n",
    "    def lookup(self, key):\n",
    "        \"\"\"\n",
    "        Return the list of values in the RDD for key `key`. This operation\n",
    "        is done efficiently if the RDD has a known partitioner by only\n",
    "        searching the partition that the key maps to.\n",
    "\n",
    "        >>> l = range(1000)\n",
    "        >>> rdd = sc.parallelize(zip(l, l), 10)\n",
    "        >>> rdd.lookup(42)  # slow\n",
    "        [42]\n",
    "        >>> sorted = rdd.sortByKey()\n",
    "        >>> sorted.lookup(42)  # fast\n",
    "        [42]\n",
    "        >>> sorted.lookup(1024)\n",
    "        []\n",
    "        >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n",
    "        >>> list(rdd2.lookup(('a', 'b'))[0])\n",
    "        ['c']\n",
    "        \"\"\"\n",
    "        values = self.filter(lambda kv: kv[0] == key).values()\n",
    "\n",
    "        if self.partitioner is not None:\n",
    "            return self.ctx.runJob(values, lambda x: x, [self.partitioner(key)])\n",
    "\n",
    "        return values.collect()\n",
    "\n",
    "    def _to_java_object_rdd(self):\n",
    "        \"\"\" Return a JavaRDD of Object by unpickling\n",
    "\n",
    "        It will convert each Python object into Java object by Pyrolite, whenever the\n",
    "        RDD is serialized in batch or not.\n",
    "        \"\"\"\n",
    "        rdd = self._pickled()\n",
    "        return self.ctx._jvm.SerDeUtil.pythonToJava(rdd._jrdd, True)\n",
    "\n",
    "    def countApprox(self, timeout, confidence=0.95):\n",
    "        \"\"\"\n",
    "        .. note:: Experimental\n",
    "\n",
    "        Approximate version of count() that returns a potentially incomplete\n",
    "        result within a timeout, even if not all tasks have finished.\n",
    "\n",
    "        >>> rdd = sc.parallelize(range(1000), 10)\n",
    "        >>> rdd.countApprox(1000, 1.0)\n",
    "        1000\n",
    "        \"\"\"\n",
    "        drdd = self.mapPartitions(lambda it: [float(sum(1 for i in it))])\n",
    "        return int(drdd.sumApprox(timeout, confidence))\n",
    "\n",
    "    def sumApprox(self, timeout, confidence=0.95):\n",
    "        \"\"\"\n",
    "        .. note:: Experimental\n",
    "\n",
    "        Approximate operation to return the sum within a timeout\n",
    "        or meet the confidence.\n",
    "\n",
    "        >>> rdd = sc.parallelize(range(1000), 10)\n",
    "        >>> r = sum(range(1000))\n",
    "        >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n",
    "        True\n",
    "        \"\"\"\n",
    "        jrdd = self.mapPartitions(lambda it: [float(sum(it))])._to_java_object_rdd()\n",
    "        jdrdd = self.ctx._jvm.JavaDoubleRDD.fromRDD(jrdd.rdd())\n",
    "        r = jdrdd.sumApprox(timeout, confidence).getFinalValue()\n",
    "        return BoundedFloat(r.mean(), r.confidence(), r.low(), r.high())\n",
    "\n",
    "    def meanApprox(self, timeout, confidence=0.95):\n",
    "        \"\"\"\n",
    "        .. note:: Experimental\n",
    "\n",
    "        Approximate operation to return the mean within a timeout\n",
    "        or meet the confidence.\n",
    "\n",
    "        >>> rdd = sc.parallelize(range(1000), 10)\n",
    "        >>> r = sum(range(1000)) / 1000.0\n",
    "        >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n",
    "        True\n",
    "        \"\"\"\n",
    "        jrdd = self.map(float)._to_java_object_rdd()\n",
    "        jdrdd = self.ctx._jvm.JavaDoubleRDD.fromRDD(jrdd.rdd())\n",
    "        r = jdrdd.meanApprox(timeout, confidence).getFinalValue()\n",
    "        return BoundedFloat(r.mean(), r.confidence(), r.low(), r.high())\n",
    "\n",
    "    def countApproxDistinct(self, relativeSD=0.05):\n",
    "        \"\"\"\n",
    "        .. note:: Experimental\n",
    "\n",
    "        Return approximate number of distinct elements in the RDD.\n",
    "\n",
    "        The algorithm used is based on streamlib's implementation of\n",
    "        `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n",
    "        of The Art Cardinality Estimation Algorithm\", available here\n",
    "        <http://dx.doi.org/10.1145/2452376.2452456>`_.\n",
    "\n",
    "        :param relativeSD: Relative accuracy. Smaller values create\n",
    "                           counters that require more space.\n",
    "                           It must be greater than 0.000017.\n",
    "\n",
    "        >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
    "        >>> 900 < n < 1100\n",
    "        True\n",
    "        >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
    "        >>> 16 < n < 24\n",
    "        True\n",
    "        \"\"\"\n",
    "        if relativeSD < 0.000017:\n",
    "            raise ValueError(\"relativeSD should be greater than 0.000017\")\n",
    "        # the hash space in Java is 2^32\n",
    "        hashRDD = self.map(lambda x: portable_hash(x) & 0xFFFFFFFF)\n",
    "        return hashRDD._to_java_object_rdd().countApproxDistinct(relativeSD)\n",
    "\n",
    "    def toLocalIterator(self):\n",
    "        \"\"\"\n",
    "        Return an iterator that contains all of the elements in this RDD.\n",
    "        The iterator will consume as much memory as the largest partition in this RDD.\n",
    "\n",
    "        >>> rdd = sc.parallelize(range(10))\n",
    "        >>> [x for x in rdd.toLocalIterator()]\n",
    "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        \"\"\"\n",
    "        with SCCallSiteSync(self.context) as css:\n",
    "            port = self.ctx._jvm.PythonRDD.toLocalIteratorAndServe(self._jrdd.rdd())\n",
    "        return _load_from_socket(port, self._jrdd_deserializer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
