{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习目标\n",
    "通过本模块，熟悉Spark RDD 常用的API操作，并通过熟悉这些API了解Spark大数据分析的相关思想。\n",
    "\n",
    "## 参考资料\n",
    "1. [开源SparkDemo](https://github.com/baifendian/SparkDemo)\n",
    "2. [Spark官方快速入门文档](http://spark.apache.org/docs/latest/quick-start.htm)\n",
    "3. [Spark官方编程指导文档](http://spark.apache.org/docs/latest/rdd-programming-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\xe5\\x88\\x9d\\xe5\\xa7\\x8b\\xe5\\x8c\\x96\\xe6\\x88\\x90\\xe5\\x8a\\x9f\\xef\\xbc\\x9asc = ', <SparkContext master=local appName=appName>)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Time         : 2017/12/26 13:32\n",
    "# @Author       : zenghao\n",
    "\n",
    "from pyspark import SparkContext, SparkConf # 导入相关工具包\n",
    "\n",
    "# 初始化Spark上下文\n",
    "# local为本地调试模式，具体集群方式参照http://spark.apache.org/docs/latest/cluster-overview.html\n",
    "conf = SparkConf().setAppName(\"appName\").setMaster(\"local\") \n",
    "sc = SparkContext(conf=conf)\n",
    "print (\"init complete：sc = \", sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data =', [1, 2, 3, 4, 5])\n",
      "('distData = ', ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:489)\n",
      "DDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).\n",
      "All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n",
      "By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.\n",
      "Local vs. cluster modes\n",
      "The behavior of the above code is undefined, and may not work as intended. To execute jobs, Spark breaks up the processing of RDD operations into tasks, each of which is executed by an executor. Prior to execution, Spark computes the task’s closure. The closure is those variables and methods which must be visible for the executor to perform its computations on the RDD (in this case foreach()). This closure is serialized and sent to each executor.\n",
      "The variables within the closure sent to each executor are now copies and thus, when counter is referenced within the foreach function, it’s no longer the counter on the driver node. There is still a counter in the memory of the driver node but this is no longer visible to the executors! The executors only see the copy from the serialized closure. Thus, the final value of counter will still be zero since all operations on counter were referencing the value within the serialized closure.\n",
      "In local mode, in some circumstances the foreach function will actually execute within the same JVM as the driver and will reference the same original counter, and may actually update it.\n",
      "To ensure well-defined behavior in these sorts of scenarios one should use an Accumulator. Accumulators in Spark are used specifically to provide a mechanism for safely updating a variable when execution is split up across worker nodes in a cluster. The Accumulators section of this guide discusses these in more detail.\n",
      "In general, closures - constructs like loops or locally defined methods, should not be used to mutate some global state. Spark does not define or guarantee the behavior of mutations to objects referenced from outside of closures. Some code that does this may work in local mode, but that’s just by accident and such code will not behave as expected in distributed mode. Use an Accumulator instead if some global aggregation is needed.\n",
      "Printing elements of an RDD\n",
      "Another common idiom is attempting to print out the elements of an RDD using rdd.foreach(println) or rdd.map(println). On a single machine, this will generate the expected output and print all the RDD’s elements. However, in cluster mode, the output to stdout being called by the executors is now writing to the executor’s stdout instead, not the one on the driver, so stdout on the driver won’t show these! To print all elements on the driver, one can use the collect() method to first bring the RDD to the driver node thus: rdd.collect().foreach(println). This can cause the driver to run out of memory, though, because collect() fetches the entire RDD to a single machine; if you only need to print a few elements of the RDD, a safer approach is to use the take(): rdd.take(100).foreach(println).\n"
     ]
    }
   ],
   "source": [
    "# 熟悉文件加载相关操作\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# 将数据加载到Spark 内存\n",
    "# 方式一：通过parallelize并行化\n",
    "distData = sc.parallelize(data)\n",
    "print (\"data =\", data)\n",
    "print (\"distData = \", distData)\n",
    "\n",
    "# 方式二：通过外部文件加载，文件存储位置可涵盖local file system, HDFS, Cassandra, HBase, Amazon S3等\n",
    "# fileData = sc.textFile(\"data/wordcount.txt\")  # 不设置前缀默认从hdfs上加载\n",
    "fileData = sc.textFile(\"file:///root/notebook/data/wordcount.txt\")  # 从本地文件系统读取\n",
    "for line in fileData.collect():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# 熟悉Spark RDD 常用API\n",
    "# python 文档详细参考：http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD\n",
    "# 下面作业需要将data通过特定操作，转换未distData\n",
    "# 实例：\n",
    "data = sc.parallelize([1, 2, 3, 4, 5])\n",
    "myData = None\n",
    "# map(func) \tReturn a new distributed dataset formed by passing each element of the source through a function func.\n",
    "# TODO 将mapData通过map函数转换未mapDistData\n",
    "myData = data.map(lambda x: x * 2).collect()  # 通过collect操作转换为正常的Python数据\n",
    "distData = [2, 4, 6, 8, 10]\n",
    "print(\"task:myData == distData should be True\", myData == distData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8]\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# filter(func) \tReturn a new dataset formed by selecting those elements of the source on which func returns true.\n",
    "data = sc.parallelize([i for i in range(10)])\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data.filter(lambda x: x % 2 == 0).collect()\n",
    "\n",
    "print(myData)\n",
    "distData = [0, 2, 4, 6, 8]\n",
    "print(\"task:myData == distData should be True\", myData == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8, 20, 22, 24, 26, 28]\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# flatMap(func) \tSimilar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).\n",
    "data = sc.parallelize([[i * 10 + j for j in range(5)] for i in range(2)])\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data.flatMap(lambda x: map(lambda y: y * 2, x)).collect()\n",
    "\n",
    "print(myData)\n",
    "distData = [0, 2, 4, 6, 8, 20, 22, 24, 26, 28]\n",
    "print(\"task:myData == distData should be True\", myData == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 7]\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# mapPartitions(func) \tSimilar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator<T> => Iterator<U> when running on an RDD of type T.\n",
    "data = sc.parallelize([1, 2, 3, 4], 2)\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "def f(iterator): yield sum(iterator)\n",
    "myData = data.mapPartitions(f).collect()\n",
    "\n",
    "print(myData)\n",
    "distData = [3, 7]\n",
    "print(\"task:myData == distData should be True\", myData == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HelloWorld\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# union(otherDataset) \tReturn a new dataset that contains the union of the elements in the source dataset and the argument.\n",
    "data1 = sc.parallelize([\"H\", \"e\", \"l\", \"l\", \"o\"])\n",
    "data2 = sc.parallelize([\"W\", \"o\", \"r\", \"l\", \"d\"])\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data1.union(data2).collect()\n",
    "myData = \"\".join(myData)\n",
    "print(myData)\n",
    "distData = \"HelloWorld\"\n",
    "print(\"task:myData == distData should be True\", myData == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', 'l']\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# intersection(otherDataset) \tReturn a new RDD that contains the intersection of elements in the source dataset and the argument.\n",
    "data1 = sc.parallelize([\"H\", \"e\", \"l\", \"l\", \"o\"])\n",
    "data2 = sc.parallelize([\"W\", \"o\", \"r\", \"l\", \"d\"])\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data1.intersection(data2).collect()\n",
    "print(myData)\n",
    "distData = ['o', 'l']\n",
    "print(\"task:myData == distData should be True\", myData == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'o']\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# distinct([numTasks])) \tReturn a new dataset that contains the distinct elements of the source dataset.\n",
    "data = sc.parallelize([\"H\", \"e\", \"l\", \"l\", \"o\"])\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data1.distinct().collect()\n",
    "print(myData)\n",
    "distData = ['H', 'e', 'l', 'o']\n",
    "print(\"task:myData == distData should be True\", myData == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, [0, 2, 4]), (1, [1, 3])]\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# groupByKey([numTasks]) \tWhen called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs.\n",
    "# Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance.\n",
    "# Note: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional numTasks argument to set a different number of tasks.\n",
    "\n",
    "data = sc.parallelize([i for i in range(5)])\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data.groupBy(lambda x: x % 2).collect()\n",
    "myData = sorted([(x, sorted(y)) for (x, y) in myData])\n",
    "\n",
    "print(myData)\n",
    "distData = [(0, [0, 2, 4]), (1, [1, 3])]\n",
    "print(\"task:myData == distData should be True\", myData == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 3), ('c', 9), ('b', 6)]\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# reduceByKey(func, [numTasks]) \tWhen called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.\n",
    "data = sc.parallelize([[(chr(ord('a') + i),i + j) for i in xrange(3)] for j in xrange(3)])\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data.flatMap(lambda x: x).reduceByKey(lambda x, y: x + y).collect()\n",
    "\n",
    "print(myData)\n",
    "distData = [('a', 3), ('c', 9), ('b', 6)]\n",
    "print(\"task:myData == distData should be True\", myData == distData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) \tWhen called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral \"zero\" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.\n",
    "import random\n",
    "data = sc.parallelize([1, 2, 3, 4, 5, 6], 3)\n",
    "\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data.aggregate(0, lambda x, y: max(x, y), lambda x, y: x + y)\n",
    "\n",
    "print(myData)\n",
    "distData = 12\n",
    "print(\"task:myData == distData should be True\", myData == distData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 6), (2, 4), (2, 7), (3, 1), (4, 5), (6, 2)]\n",
      "[(2, 7), (1, 6), (4, 5), (2, 4), (6, 2), (3, 1)]\n",
      "('task:myData1 == distData1 should be True', True)\n",
      "('task:myData2 == distData2 should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# sortBy(keyfunc, ascending=True, numPartitions=None) Sorts this RDD by the given keyfunc\n",
    "data = sc.parallelize([(3, 1), (2, 4), (6, 2), (1, 6), (4, 5), (2, 7)])\n",
    "myData1 = None\n",
    "myData2 = None\n",
    "\n",
    "# Add your code here\n",
    "myData1 = data.sortBy(lambda x: x[0]).collect()\n",
    "myData2 = data.sortBy(lambda x: x[1], False).collect()\n",
    "print(myData1)\n",
    "print(myData2)\n",
    "\n",
    "distData1 = [(1, 6), (2, 4), (2, 7), (3, 1), (4, 5), (6, 2)]\n",
    "distData2 = [(2, 7), (1, 6), (4, 5), (2, 4), (6, 2), (3, 1)]\n",
    "print(\"task:myData1 == distData1 should be True\", myData1 == distData1)\n",
    "print(\"task:myData2 == distData2 should be True\", myData2 == distData2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 7), ('b', 11)]\n",
      "('task:myData == distData should be True', True)\n"
     ]
    }
   ],
   "source": [
    "# join(otherDataset, [numTasks]) \tWhen called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.\n",
    "data1 = sc.parallelize([(\"a\", 1), (\"b\", 4),(\"b\",1)])\n",
    "data2 = sc.parallelize([(\"a\", 2), (\"a\", 3), (\"b\", 3)])\n",
    "myData = None\n",
    "\n",
    "# Add your code here\n",
    "myData = data1.join(data2).reduceByKey(lambda x, y: sum(x) + sum(y)).collect()\n",
    "print(myData)\n",
    "\n",
    "distData = [('a', 7), ('b', 11)]\n",
    "print(\"task:myData == distData should be True\", myData == distData)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
